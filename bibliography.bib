% This file was created with JabRef 2.7b.
% Encoding: UTF-8

@INPROCEEDINGS{Archambeau:2006,
  author = {Archambeau, C\'{e}dric and Delannay, Nicolas and Verleysen, Michel},
  title = {Robust probabilistic projections},
  pages = {33--40},
  crossref = {ICML:2006}
}

@INCOLLECTION{Barber:2007,
  author = {Barber, David and Chiappa, Silvia},
  title = {Unified Inference for Variational {Bayesian} Linear {Gaussian} State-Space
	Models},
  crossref = {NIPS:2006}
}

@INPROCEEDINGS{Bishop:1999,
  author = {Bishop, Christopher M.},
  title = {Variational principal components},
  pages = {509--514},
  abstract = {testi vaan},
  crossref = {ICANN:1999}
}

@INCOLLECTION{Bonilla:2008,
  author = {Bonilla, Edwin V. and Chai, Kian Ming A. and Williams, Christopher
	K. I.},
  title = {Multi-task {Gaussian} Process Regression},
  crossref = {NIPS:2007}
}

@INCOLLECTION{Boyle:2005,
  author = {Boyle, Phillip and Frean, Marcus},
  title = {Dependent {Gaussian} Processes},
  crossref = {NIPS:2004}
}

@INCOLLECTION{Collins:2002,
  author = {M. Collins and S. Dasgupta and R. Schapire},
  title = {A generalization of principal components analysis to the exponential
	family},
  pages = {617--624},
  crossref = {NIPS:2001}
}

@INCOLLECTION{Griffiths:2006,
  author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
  title = {Infinite Latent Feature Models and the {Indian} Buffet Process},
  pages = {475--482},
  crossref = {NIPS:2005}
}

@INCOLLECTION{Honkela:2005,
  author = {A. Honkela and H. Valpola},
  title = {Unsupervised Variational {B}ayesian Learning of Nonlinear Models},
  pages = {593--600},
  crossref = {NIPS:2004}
}

@INCOLLECTION{Lazaro-Gredilla:2009,
  author = {L\'{a}zaro-Gredilla, Miguel and Figueiras-Vidal, An\'{i}bal R.},
  title = {Inter-domain {Gaussian} Processes for Sparse Inference using Inducing
	Features},
  abstract = {We present a general inference framework for inter-domain Gaussian
	Processes (GPs) and focus on its usefulness to build sparse GP models.
	The state-of-the-art sparse GP model introduced by Snelson and Ghahramani
	in [1] relies on finding a small, representative pseudo data set
	of m elements (from the same domain as the n available data elements)
	which is able to explain existing data well, and then uses it to
	perform inference. This reduces inference and model selection computation
	time from O(n3) to O(m2n), where m n. Inter-domain GPs can be used
	to find a (possibly more compact) representative set of features
	lying in a different domain, at the same computational cost. Being
	able to specify a different domain for the representative features
	allows to incorporate prior knowledge about relevant characteristics
	of data and detaches the functional form of the covariance and basis
	functions. We will show how previously existing models fit into this
	framework and will use it to develop two new sparse GP models. Tests
	on large, representative regression data sets suggest that significant
	improvement can be achieved, while retaining computational efficiency.},
  crossref = {NIPS:2009}
}

@INPROCEEDINGS{Lakshminarayanan:2011,
  author = {Lakshminarayanan, Balaji and Bouchard, Guillaume and Archambeau,
	Cedric},
  title = {Robust {Bayesian} Matrix Factorisation},
  crossref = {AISTATS:2011}
}

@INCOLLECTION{Lawrence:2004,
  author = {Lawrence, Neil D.},
  title = {Gaussian Process Latent Variable Models for Visualisation of High
	Dimensional Data},
  pages = {329--336},
  crossref = {NIPS:2003}
}

@INPROCEEDINGS{Lawrence:2002,
  author = {Lawrence, Neil D. and Seeger, Matthias and Herbrich, Ralf},
  title = {Fast sparse {Gaussian} process methods: the informative vector machine},
  crossref = {NIPS:2002}
}

@INPROCEEDINGS{Lawrence:2009,
  author = {Neil D. Lawrence and Raquel Urtasun},
  title = {Non-linear Matrix Factorization with {Gaussian} Processes},
  pages = {601--608},
  crossref = {ICML:2009}
}

@INCOLLECTION{Luttinen:2009:nips,
  author = {Luttinen, Jaakko and Ilin, Alexander},
  title = {Variational {Gaussian}-process factor analysis for modeling spatio-temporal
	data},
  crossref = {NIPS:2009}
}

@INPROCEEDINGS{Luttinen:2009:ica,
  author = {Luttinen, Jaakko and Ilin, Alexander and Karhunen, Juha},
  title = {Bayesian Robust {PCA} for Incomplete Data},
  pages = {66--73},
  crossref = {ICA:2009},
  doi = {10.1007/978-3-642-00599-2_9}
}

@INPROCEEDINGS{Luttinen:2009:esann,
  author = {Luttinen, Jaakko and Ilin, Alexander and Raiko, Tapani},
  title = {Transformations for Variational Factor Analysis to Speed up Learning},
  pages = {77--82},
  crossref = {ESANN:2009}
}

@INCOLLECTION{Minka:2001,
  author = {Minka, Thomas P.},
  title = {Automatic Choice of Dimensionality for {PCA}},
  pages = {598-604},
  crossref = {NIPS:2000}
}

@INCOLLECTION{Mohamed:2008,
  author = {Mohamed, Shakir and Heller, Katherine and Ghahramani, Zoubin},
  title = {Bayesian Exponential Family {PCA}},
  pages = {1089--1096},
  crossref = {NIPS:2008}
}

@INCOLLECTION{Murray:2010,
  author = {Murray, Iain and Adams, Ryan Prescott},
  title = {Slice sampling covariance hyperparameters of latent {Gaussian} models},
  crossref = {NIPS:2010}
}

@INPROCEEDINGS{Orbanz:2009,
  author = {Orbanz, Peter},
  title = {Construction of Nonparametric {Bayesian} Models from Parametric {Bayes}
	Equations},
  note = {To appear},
  crossref = {NIPS:2009}
}

@INPROCEEDINGS{Pang:2007,
  author = {Pang, Junbiao and Qing, Laiyun and Huang, Qingming and Jiang, Shuqiang
	and Gao, Wen},
  title = {Monocular Tracking {3D} People By {Gaussian} Process Spatio-Temporal
	Variable Model},
  pages = {41--44},
  crossref = {ICIP:2007}
}

@INCOLLECTION{Qi:2007,
  author = {Qi, Yuan (Alan) and Jaakkola, Tommi S.},
  title = {Parameter Expanded Variational {Bayesian} Methods},
  pages = {1097--1104},
  crossref = {NIPS:2006}
}

@INPROCEEDINGS{Schmidt:2009,
  author = {Schmidt, Mikkel N.},
  title = {Function factorization using warped {Gaussian} processes},
  pages = {921--928},
  crossref = {ICML:2009}
}

@INPROCEEDINGS{Seeger:2003,
  author = {Seeger, Matthias and Williams, Christopher K. I. and Lawrence, Neil
	D.},
  title = {Fast Forward Selection to Speed Up Sparse Gaussian Process Regression},
  pages = {205--213},
  crossref = {AISTATS:2003}
}

@INPROCEEDINGS{Smola:2001,
  author = {Smola, Alex J. and Bartlett, Peter},
  title = {Sparse greedy {Gaussian} process regression},
  crossref = {NIPS:2000}
}

@INCOLLECTION{Snelson:2006,
  author = {Snelson, Edward and Ghahramani, Zoubin},
  title = {Sparse Gaussian Processes using Pseudo-inputs},
  pages = {1257--1264},
  crossref = {NIPS:2005}
}

@INPROCEEDINGS{Snelson:2007,
  author = {Snelson, Edward and Ghahramani, Zoubin},
  title = {Local and global sparse {Gaussian} process approximations},
  crossref = {AISTATS:2007}
}

@INPROCEEDINGS{Teh:2005,
  author = {Teh, Yee Whye and Seeger, Matthias and Jordan, Michael I.},
  title = {Semiparametric Latent Factor Models},
  pages = {333--340},
  crossref = {AISTATS:2005}
}

@INPROCEEDINGS{Titsias:2009,
  author = {Titsias, Michalis K.},
  title = {Variational Learning of Inducing Variables in Sparse {Gaussian} Processes},
  pages = {567--574},
  crossref = {AISTATS:2009}
}

@INCOLLECTION{Tresp:2001,
  author = {Tresp, Volker},
  title = {Mixtures of {Gaussian} Processes},
  pages = {654--660},
  crossref = {NIPS:2000}
}

@INPROCEEDINGS{Vanhatalo:2008,
  author = {Vanhatalo, Jarno and Vehtari, Aki},
  title = {Modelling local and global phenomena with sparse {Gaussian} processes},
  pages = {571--578},
  crossref = {UAI:2008}
}

@INPROCEEDINGS{Williams:1996,
  author = {Williams, Christopher K. I. and Rasmussen, Carl Edward},
  title = {Gaussian Processes for Regression},
  crossref = {NIPS:1995}
}

@INPROCEEDINGS{Williams:2001,
  author = {Williams, Christopher K. I. and Seeger, Matthias},
  title = {Using the {Nystr\"om} method to speed up kernel machines},
  crossref = {NIPS:2000}
}

@INCOLLECTION{Wright:2009,
  author = {Wright, John and Peng, Yigang and Ma, Yi and Ganesh, Arvind and Rao,
	Shankar},
  title = {Robust Principal Component Analysis: Exact Recovery of Corrupted
	Low-Rank Matrices by Convex Optimization},
  crossref = {NIPS:2009}
}

@INCOLLECTION{Yu:2009,
  author = {Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu,
	Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
  title = {Gaussian-process factor analysis for low-dimensional single-trial
	analysis of neural population activity},
  pages = {1881--1888},
  crossref = {NIPS:2008}
}

@INPROCEEDINGS{Smidl:2003,
  author = {\vSm\´idl, V\´aclav and A. Quinn},
  title = {Fast variational {PCA} for functional analysis of dynamic image sequences},
  booktitle = {Proceedings of the 3rd International Symposium on Image and Signal
	Processing and Analysis (ISPA'2003)},
  year = {2003},
  volume = {1},
  pages = {555--560},
  month = {September}
}

@BOOK{Banerjee:2004,
  title = {Hierarchical Modeling and Analysis for Spatial Data},
  publisher = {Chapman and Hall/CRC},
  year = {2004},
  author = {Banerjee, Sudipto and Carlin, Bradley P. and Gelfand, Alan E.},
  series = {Monographs on Statistics and Applied Probability}
}

@BOOK{Bar-Shalom:2001,
  title = {Estimation with Applications to Tracking and Navigation},
  publisher = {Wiley-Interscience},
  year = {2001},
  author = {Bar-Shalom, Y and Li, X Rong and Kirubarajan, T}
}

@PHDTHESIS{Beal:2003,
  author = {Beal, Matthew J.},
  title = {Variational algorithms for approximate {Bayesian} inference},
  school = {Gatsby Computational Neuroscience Unit, University College London},
  year = {2003}
}

@ARTICLE{Beal:2003b,
  author = {Beal, M. J. and Ghahramani, Z.},
  title = {The Variational {Bayesian} {EM} Algorithm for Incomplete Data: with
	Application to Scoring Graphical Model Structures},
  journal = {Bayesian Statistics},
  year = {2003},
  volume = {7},
  pages = {453--464},
  owner = {jluttine},
  timestamp = {2013.04.19}
}

@BOOK{Bishop:2006,
  title = {Pattern Recognition and Machine Learning},
  publisher = {Springer},
  year = {2006},
  author = {Bishop, Christopher M.},
  series = {Information Science and Statistics},
  address = {New York},
  edition = {2nd}
}

@BOOK{Bottomley:1990,
  title = {Global Ocean Surface Temperature Atlas},
  publisher = {Her Majesty's Stn. Off.},
  year = {1990},
  author = {Bottomley, M. and Folland, C. K. and Hsiung, J. and Newell, R. E.
	and Parker, D. E.},
  address = {Norwich, England}
}

@ARTICLE{Calder:2007,
  author = {Calder, Catherine A.},
  title = {Dynamic factor process convolution models for multivariate space-time
	data with application to air quality assessment},
  journal = {Environmental and Ecological Statistics},
  year = {2007},
  volume = {14},
  pages = {229--247},
  number = {3}
}

@ARTICLE{Candes:2011,
  author = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  title = {Robust Principal Component Analysis?},
  journal = {Journal of the ACM},
  year = {2011},
  volume = {58},
  pages = {37}
}

@INPROCEEDINGS{Chandrasekaran:2009,
  author = {Chandrasekaran, Venkat and Sanghavi, Sujay and Parrilo, Pablo A.
	and Willsky, Alan S.},
  title = {Sparse and Low-Rank Matrix Decomposition},
  booktitle = {IFAC Symposium on System Identification},
  year = {2009}
}

@PHDTHESIS{Charles:2009,
  author = {Charles, Wilson Mahera},
  title = {Stochastic Differential Equations: Applications in Transport modelling
	in shallow waters},
  school = {Lappeenranta University of Technology},
  year = {2009}
}

@ARTICLE{Clark:2005,
  author = {Clark, James S.},
  title = {Why environmental scientists are becoming {Bayesians}},
  journal = {Ecology Letters},
  year = {2005},
  volume = {8},
  pages = {2--14}
}

@BOOK{Cressie:1993,
  title = {Statistics for Spatial Data},
  publisher = {Wiley},
  year = {1993},
  author = {Cressie, Noel},
  series = {Wiley Series in Probability and Mathematical Statistics},
  address = {New York}
}

@ARTICLE{Cressie:1999,
  author = {Cressie, Noel and Huang, Hsin-Cheng},
  title = {Classes of Nonseparable, Spatio-Temporal Stationary Covariance Functions},
  journal = {Journal of the American Statistical Association},
  year = {1999},
  volume = {94},
  pages = {1330--1340},
  number = {448},
  month = dec
}

@ARTICLE{Csato:2002,
  author = {Csató, Lehel and Opper, Manfred},
  title = {Sparse online {Gaussian} processes},
  journal = {Neural Computation},
  year = {2002},
  volume = {14},
  pages = {641--668},
  number = {3}
}

@ARTICLE{DeCesare:2001,
  author = {De Cesare, L. and Myers, D. E. and Posa, D.},
  title = {Estimating and modeling space-time correlation structures},
  journal = {Statistics \& Probability Letters},
  year = {2001},
  volume = {51},
  pages = {9--14}
}

@ARTICLE{DeIaco:2001,
  author = {De Iaco, S. and Myers, D. E. and Posa, D.},
  title = {Space-time analysis using a general product-sum model},
  journal = {Statistics \& Probability Letters},
  year = {2001},
  volume = {52},
  pages = {21--28}
}

@ARTICLE{Ding:2011,
  author = {Ding, Xinghao and He, Lihan and Carin, Lawrence},
  title = {Bayesian Robust Principal Component Analysis},
  journal = {IEEE Transactions on Image Processing},
  year = {2011},
  volume = {20},
  pages = {3419--3430},
  number = {12}
}

@MISC{Doucet:2010,
  author = {Doucet, Arnaud},
  title = {A Note on Efficient Conditional Simulation of {Gaussian} Distributions}
}

@ARTICLE{Duan:2009,
  author = {Duan, Jason A. and Gelfand, Alan E. and Sirmans, C. F.},
  title = {Modeling Space-Time Data Using Stochastic Differential Equations},
  journal = {Bayesian Analysis},
  year = {2009},
  volume = {4},
  pages = {733--758},
  number = {4},
  abstract = {This paper demonstrates the use and value of stochastic differential
	equations for modeling space-time data in two common settings. The
	first consists of point-referenced or geostatistical data where observations
	are collected at fixed locations and times. The second considers
	random point pattern data where the emergence of locations and times
	is random. For both cases, we employ stochastic differential equations
	to describe a latent process within a hierarchical model for the
	data. The intent is to view this latent process mechanistically and
	endow it with appropriate simple features and interpretable parameters.
	A motivating problem for the second setting is to model urban development
	through observed locations and times of new home construction; this
	gives rise to a space-time point pattern. We show that a spatio-temporal
	Cox process whose intensity is driven by a stochastic logistic equation
	is a viable mechanistic model that affords meaningful interpretation
	for the results of statistical inference. Other applications of stochastic
	logistic differential equations with space-time varying parameters
	include modeling population growth and product diffusion, which motivate
	our first, point-referenced data application. We propose a method
	to discretize both time and space in order to fit the model. We demonstrate
	the inference for the geostatistical model through a simulated dataset.
	Then, we fit the Cox process model to a real dataset taken from the
	greater Dallas metropolitan area.},
  doi = {10.1214/09-BA427}
}

@ARTICLE{vanDyk:2001,
  author = {van Dyk, David A. and Meng, Xiao-Li},
  title = {The Art of Data Augmentation},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2001},
  volume = {10},
  pages = {1--50},
  number = {1}
}

@ARTICLE{Eubank:2002,
  author = {Eubank, R. L. and Wang, Suojin},
  title = {The Equivalence Between the {Cholesky} Decomposition and the {Kalman}
	Filter},
  journal = {The American Statistician},
  year = {2002},
  volume = {56},
  pages = {39--43},
  number = {1},
  owner = {jluttine},
  timestamp = {2013.04.19}
}

@ARTICLE{Fritz:2009,
  author = {Fritz, J. and Neuweiler, I. and Nowak, W.},
  title = {Application of {FFT}-based Algorithms for Large-Scale Universal Kriging
	Problems},
  journal = {Mathematical Geosciences},
  year = {2009},
  volume = {41},
  pages = {509--533},
  number = {5},
  abstract = {Looking at kriging problems with huge numbers of estimation points
	and measurements, computational power and storage capacities often
	pose heavy limitations to the maximum manageable problem size. In
	the past, a list of FFT-based algorithms for matrix operations have
	been developed. They allow extremely fast convolution, superposition
	and inversion of covariance matrices under certain conditions. If
	adequately used in kriging problems, these algorithms lead to drastic
	speedup and reductions in storage requirements without changing the
	kriging estimator. However, they require second-order stationary
	covariance functions, estimation on regular grids, and the measurements
	must also form a regular grid. In this study, we show how to alleviate
	these rather heavy and many times unrealistic restrictions. Stationarity
	can be generalized to intrinsicity and beyond, if decomposing kriging
	problems into the sum of a stationary problem and a formally decoupled
	regression task. We use universal kriging, because it covers arbitrary
	forms of unknown drift and all cases of generalized covariance functions.
	Even more general, we use an extension to uncertain rather than unknown
	drift coefficients. The sampling locations may now be irregular,
	but must form a subset of the estimation grid. Finally, we present
	asymptotically exact but fast approximations to the estimation variance
	and point out application to conditional simulation, cokriging and
	sequential kriging. The drastic gain in computational and storage
	efficiency is demonstrated in test cases. Especially high-resolution
	and data-rich fields such as rainfall interpolation from radar measurements
	or seismic or other geophysical inversion can benefit from these
	improvements.},
  doi = {10.1007/s11004-009-9220-x}
}

@ARTICLE{Fuentes:2006,
  author = {Fuentes, Montserrat},
  title = {Testing for separability of spatial-temporal covariance functions},
  journal = {Journal of Statistical Planning and Inference},
  year = {2006},
  volume = {136},
  pages = {447--466}
}

@ARTICLE{Furrer:2006,
  author = {Furrer, Reinhard and Genton, Marc G. and Nychka, Douglas},
  title = {Covariance Tapering for Interpolation of Large Spatial Datasets},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2006},
  volume = {15},
  pages = {502--523}
}

@ARTICLE{Gao:2008,
  author = {Gao, Junbin},
  title = {Robust {L1} Principal Component Analysis and Its {Bayesian} variational
	inference},
  journal = {Neural computation},
  year = {2008},
  volume = {20},
  pages = {555--572},
  number = {2}
}

@BOOK{Gelman:2003,
  title = {Bayesian Data Analysis},
  publisher = {Chapman and Hall/CRC},
  year = {2003},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald
	B.},
  series = {Texts in Statistical Science},
  address = {Florida},
  edition = {2nd}
}

@ARTICLE{Genton:2007,
  author = {Genton, Marc G.},
  title = {Separable approximations of space-time covariance matrices},
  journal = {Environmetrics},
  year = {2007},
  volume = {18},
  pages = {681--695}
}

@ARTICLE{Girolami:2008,
  author = {Girolami, Mark},
  title = {{Bayesian} inference for differential equations},
  journal = {Theoretical Computer Science},
  year = {2008},
  volume = {408},
  pages = {4--16},
  number = {1},
  abstract = {Nonlinear dynamic systems such as biochemical pathways can be represented
	in abstract form using a number of modelling formalisms. In particular
	differential equations provide a highly expressive mathematical framework
	with which to model dynamic systems, and a very natural way to model
	the dynamics of a biochemical pathway in a deterministic manner is
	through the use of nonlinear ordinary or time delay differential
	equations. However if, for example, we consider a biochemical pathway
	the constituent chemical species and hence the pathway structure
	are seldom fully characterised. In addition it is often impossible
	to obtain values of the rates of activation or decay which form the
	free parameters of the mathematical model. The system model in many
	cases is therefore not fully characterised either in terms of structure
	or the values which parameters take. This uncertainty must be accounted
	for in a systematic manner when the model is used in simulation or
	predictive mode to safeguard against reaching conclusions about system
	characteristics that are unwarranted, or in making predictions that
	are unjustifiably optimistic given the uncertainty about the model.
	The Bayesian inferential methodology provides a coherent framework
	with which to characterise and propagate uncertainty in such mechanistic
	models and this paper provides an introduction to Bayesian methodology
	as applied to system models represented as differential equations.},
  doi = {10.1016/j.tcs.2008.07.005}
}

@ARTICLE{Girolami:2011,
  author = {Girolami, Mark and Calderhead, Ben},
  title = {{Riemann} manifold {Langevin} and {Hamiltonian} {Monte} {Carlo} methods},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year = {2011},
  volume = {73},
  pages = {123--214},
  number = {2},
  doi = {10.1111/j.1467-9868.2010.00765.x}
}

@ARTICLE{Gneiting:2002,
  author = {Gneiting, Tilmann},
  title = {Compactly Supported Correlation Functions},
  journal = {Journal of Multivariate Analysis},
  year = {2002},
  volume = {83},
  pages = {493--508},
  doi = {10.1006/jmva.2001.2056}
}

@ARTICLE{Gneiting:2002a,
  author = {Gneiting, Tilmann},
  title = {Nonseparable, Stationary Covariance Functions for Space-Time Data},
  journal = {Journal of the American Statistical Association},
  year = {2002},
  volume = {97},
  pages = {590--600},
  number = {458}
}

@ARTICLE{Haario:2006,
  author = {Haario, Heikki and Laine, Marko and Mira, Antonietta and Saksman,
	Eero},
  title = {{DRAM:} Efficient adaptive {MCMC}},
  journal = {Statistics and Computing},
  year = {2006},
  volume = {16},
  pages = {339--354},
  doi = {10.1007/s11222-006-9438-0}
}

@ARTICLE{Hoffman:1991,
  author = {Hoffman, Yehuda and Ribak, Erez},
  title = {Constrained realizations of {Gaussian} fields: a simple algorithm},
  journal = {The Astrophysical Journal},
  year = {1991},
  volume = {380},
  pages = {L5--L8}
}

@INPROCEEDINGS{Ilin:2009,
  author = {Ilin, Alexander and Kaplan, Alexey},
  title = {{Bayesian} {PCA} for Reconstruction of Historical Sea Surface Temperatures},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks
	(IJCNN 2009)},
  year = {2009},
  pages = {1322--1327}
}

@ARTICLE{Ilin:2010:PCA,
  author = {Ilin, Alexander and Raiko, Tapani},
  title = {Practical Approaches to Principal Component Analysis in the Presence
	of Missing Values},
  journal = {Journal of Machine Learning Research},
  year = {2010},
  volume = {11},
  pages = {1957--2000}
}

@TECHREPORT{Ilin:2008,
  author = {A. Ilin and T. Raiko},
  title = {Practical Approaches to Principal Component Analysis in the Presence
	of Missing Values},
  institution = {Helsinki University of Technology},
  year = {2008},
  number = {TKK-ICS-R6},
  address = {Espoo, Finland},
  note = {Available at http://www.cis.hut.fi/alexilin/}
}

@ARTICLE{Ilin:2005,
  author = {Ilin, Alexander and Valpola, Harri},
  title = {On the Effect of the Form of the Posterior Approximation in Variational
	Learning of {ICA} Models},
  journal = {Neural Processing Letters},
  year = {2005},
  volume = {22},
  pages = {183--204},
  number = {2}
}

@ARTICLE{Ilin:2006,
  author = {Ilin, Alexander and Valpola, Harri and Oja, Erkki},
  title = {Exploratory analysis of climate data using source separation methods},
  journal = {Neural Networks},
  year = {2006},
  volume = {19},
  pages = {155--167},
  number = {2}
}

@BOOK{Jolliffe:2002,
  title = {Principal Component Analysis},
  publisher = {Springer},
  year = {2002},
  author = {Jolliffe, Ian T.},
  address = {New York},
  edition = {2nd}
}

@INPROCEEDINGS{Jordan:1998,
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S.
	and Saul, Lawrence K.},
  title = {An Introduction to Variational Methods for Graphical Methods},
  booktitle = {Machine Learning},
  year = {1998},
  pages = {183--233},
  publisher = {MIT Press}
}

@ARTICLE{Jun:2007,
  author = {Jun, Mikyoung and Stein, Michael L.},
  title = {An Approach to Producing Space–Time Covariance Functions on Spheres},
  journal = {Technometrics},
  year = {2007},
  volume = {49},
  pages = {468--479},
  number = {4},
  month = {nov},
  abstract = {For space–time processes on global or large scales, it is critical
	to use models that respect the Earth's spherical shape. The covariance
	functions of such processes should be not only positive definite
	on sphere × time, but also capable of capturing the dynamics of the
	processes well. We develop space–time covariance functions on sphere
	× time that are flexible in producing space–time interactions, especially
	space–time asymmetries. Our idea is to consider a sum of independent
	processes in which each process is obtained by applying a first-order
	differential operator to a fully symmetric process on sphere × time.
	The resulting covariance functions can produce various types of space–time
	interactions and give different covariance structures along different
	latitudes. Our approach yields explicit expressions for the covariance
	functions, which has great advantages in computation. Moreover, it
	applies equally well to generating asymmetric space–time covariance
	functions on flat or other spatial domains. We study various characteristics
	of our new covariance functions, focusing on their space–time interactions.
	We apply our model to a dataset of total column ozone levels in the
	Northern hemisphere.},
  doi = {10.1198/004017007000000155}
}

@ARTICLE{Kalman:1961,
  author = {Kalman, R. E. and Bucy, R. S.},
  title = {New Results in Linear Filtering and Prediction Theory},
  journal = {Journal of Basic Engineering},
  year = {1961},
  volume = {85},
  pages = {95--108},
  owner = {jluttine},
  timestamp = {2013.04.19}
}

@INPROCEEDINGS{Kang:2012,
  author = {Hyohyeong Kang and Seungjin Choi},
  title = {Probabilistic Models for Common Spatial Patterns: Parameter-Expanded
	{EM} and Variational {Bayes}},
  booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
  year = {2012},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/4839}
}

@ARTICLE{Kaplan:1998,
  author = {Kaplan, Alexey and Cane, Mark A. and Kushnir, Yochanan and Clement,
	Amy C. and Blumenthal, M. Benno and Rajagopalan, Balaji},
  title = {Analyses of global sea surface temperature 1856--1991},
  journal = {Journal of Geophysical Research},
  year = {1998},
  volume = {103},
  pages = {18567--18589},
  number = {C9},
  month = aug
}

@ARTICLE{Klami:2013,
  author = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
  title = {{Bayesian} canonical correlation analysis},
  journal = {Journal of Machine Learning Research},
  year = {2013},
  volume = {14},
  pages = {899--937},
  owner = {jluttine},
  timestamp = {2013.04.19}
}

@INPROCEEDINGS{Kozma:2009,
  author = {L.~Kozma and A.~Ilin and T.~Raiko},
  title = {Binary principal component analysis in the {N}etflix collaborative
	filtering task},
  booktitle = {Proceedings of the 2009 IEEE international workshop on machine learning
	for signal processing (MLSP~2009)},
  year = {2008},
  note = {To appear}
}

@INCOLLECTION{Lappalainen:2000b,
  author = {Lappalainen, Harri and Honkela, Antti},
  title = {{B}ayesian Nonlinear Independent Component Analysis by Multi-Layer
	Perceptrons},
  booktitle = {Advances in Independent Component Analysis},
  publisher = {Springer-Verlag},
  year = {2000},
  editor = {M.~Girolami},
  pages = {93--121},
  address = {Berlin}
}

@INCOLLECTION{Lappalainen:2000a,
  author = {Lappalainen, Harri and Miskin, James W.},
  title = {Ensemble Learning},
  booktitle = {Advances in Independent Component Analysis},
  publisher = {Springer-Verlag},
  year = {2000},
  editor = {M. Girolami},
  pages = {75--92}
}

@ARTICLE{Lawrence:2005,
  author = {Neil D. Lawrence},
  title = {Probabilistic Non-linear Principal Component Analysis with {Gaussian}
	Process Latent Variable Models},
  journal = {Journal of Machine Learning Research},
  year = {2005},
  volume = {6},
  pages = {1783-1816}
}

@ARTICLE{Lemos:2009,
  author = {Lemos, Ricardo T. and Sans\'{o}},
  title = {A spatio-temporal model for mean, anomaly and trend fields of {North}
	{Atlantic} sea surface temperature},
  journal = {Journal of the American Statistical Association},
  year = {2009},
  volume = {104},
  pages = {5--18},
  number = {485},
  abstract = {We consider the problem of fitting a statistical model to 30 years
	of sea surface temperature records collected over a large portion
	of the Northern Atlantic. The observations were collected sparsely
	in space and time with different levels of accuracy. The purpose
	of the model is to produce an atlas of oceanic properties, including
	climatological mean fields, estimates of historical trends, and a
	spatio-temporal reconstruction of the anomalies, i.e., the transient
	deviations from the climatological mean. These products are of interest
	to climate change and climate variability research, numerical modeling,
	and remote sensing analyses. Our model improves upon the current
	tools used by oceanographers in that it constructs instantaneous
	temperature fields before averaging them into the climatology, thus
	giving equal weight to all years in the time frame, regardless of
	the temporal distribution of data. It also accounts for nonisotropic
	and nonstationary space and time dependencies, owing to its use of
	discrete process convolutions. Particular attention is given to the
	handling of massive datasets such as the one under study. This is
	achieved by considering compact support kernels that allow an efficient
	parallelization of the Markov chain Monte Carlo method used in the
	estimation of the model parameters. Resulting monthly climatologies
	are compared with those of the World Ocean Atlas 2001, version 2.
	Different water masses appear better separated in our climatology,
	and a close link emerges between the kernels' shape and the dominating
	patterns of ocean currents. The subpolar and the temperate North
	Atlantic display opposite trends, with the former mainly cooling
	over the years and the latter mainly warming, especially in the Gulf
	Stream region. Long-term changes in annual cycles are also detected.
	As in any hierarchical Bayesian model, parameter estimates come with
	credibility intervals, which are useful to compare results with other
	approaches and detect areas where sampling campaigns are needed the
	most.},
  doi = {10.1198/jasa.2009.0018}
}

@ARTICLE{Liu:1998,
  author = {Liu, Chuanhai and Rubin, Donald B. and Wu, Ying Nian},
  title = {Parameter expansion to accelerate {EM:} the {PX-EM} algorithm},
  journal = {Biometrika},
  year = {1998},
  volume = {85},
  pages = {755--770}
}

@ARTICLE{Liu:1999,
  author = {Liu, Jun S. and Wu, Ying N.},
  title = {Parameter Expansion for Data Augmentation},
  journal = {Journal of the American Statistical Association},
  year = {1999},
  volume = {94},
  pages = {1264--1274},
  publisher = {American Statistical Association}
}

@ARTICLE{Lopes:2008,
  author = {Lopes, Hedibert Freitas and Salazar, Esther and Gamerman, Dani},
  title = {Spatial Dynamic Factor Analysis},
  journal = {Bayesian Analysis},
  year = {2008},
  volume = {3},
  pages = {759--792},
  number = {4}
}

@INPROCEEDINGS{Luttinen:2012:aistats,
  author = {Luttinen, Jaakko and Ilin, Alexander},
  title = {Efficient {Gaussian} Process Inference for Short-Scale Spatio-Temporal
	Modeling},
  booktitle = {JMLR Workshop and Conference Proceedings (AISTATS 2012)},
  year = {2012},
  volume = {22},
  pages = {741--750},
  abstract = {This paper presents an efficient Gaussian process inference scheme
	for modeling shortscale phenomena in spatio-temporal datasets. Our
	model uses a sum of separable, compactly supported covariance functions,
	which yields a full covariance matrix represented in terms of small
	sparse matrices operating either on the spatial or temporal domain.
	The proposed inference procedure is based on Gibbs sampling, in which
	samples from the conditional distribution of the latent function
	values are obtained by applying a simple linear transformation to
	samples drawn from the joint distribution of the function values
	and the observations. We make use of the proposed model structure
	and the conjugate gradient method to compute the required transformation.
	In the experimental part, the proposed algorithm is compared to the
	standard approach using the sparse Cholesky decomposition and it
	is shown to be much faster and computationally feasible for 100-1000
	times larger datasets. We demonstrate the advantages of the proposed
	method in the problem of reconstructing sea surface temperature,
	which requires processing of a real-world dataset with 10^6 observations.},
  url = {http://jmlr.csail.mit.edu/proceedings/papers/v22/luttinen12/luttinen12.pdf}
}

@ARTICLE{Luttinen:2010,
  author = {Luttinen, Jaakko and Ilin, Alexander},
  title = {Transformations in variational {Bayesian} factor analysis to speed
	up learning},
  journal = {Neurocomputing},
  year = {2010},
  volume = {73},
  pages = {1093--1102},
  abstract = {We propose simple transformation of the hidden states in variational
	Bayesian factor analysis models to speed up the learning procedure.
	The speed-up is achieved by using proper parameterization of the
	posterior approximation which allows joint optimization of its individual
	factors, thus the transformation is theoretically justified. We derive
	the transformation formulae for variational Bayesian factor analysis
	and show experimentally that it can significantly improve the rate
	of convergence. The proposed transformation basically performs centering
	and whitening of the hidden factors taking into account the posterior
	uncertainties. Similar transformations can be applied to other variational
	Bayesian factor analysis models as well.},
  doi = {10.1016/j.neucom.2009.11.018}
}

@ARTICLE{Luttinen:2012:npl,
  author = {Luttinen, Jaakko and Ilin, Alexander and Karhunen, Juha},
  title = {{Bayesian} Robust {PCA} of Incomplete Data},
  journal = {Neural Processing Letters},
  year = {2012},
  volume = {36},
  pages = {189--202},
  number = {2},
  abstract = {We present a probabilistic model for robust factor analysis and principal
	component analysis in which the observation noise is modeled by Student-
	t distributions in order to reduce the negative effect of outliers.
	The Student- t distributions are modeled independently for each data
	dimensions, which is different from previous works using multivariate
	Student- t distributions. We compare methods using the proposed noise
	distribution, the multivariate Student- t and the Laplace distribution.
	Intractability of evaluating the posterior probability density is
	solved by using variational Bayesian approximation methods. We demonstrate
	that the assumed noise model can yield accurate reconstructions because
	corrupted elements of a bad quality sample can be reconstructed using
	the other elements of the same data vector. Experiments on an artificial
	dataset and a weather dataset show that the dimensional independency
	and the flexibility of the proposed Student- t noise model can make
	it superior in some applications.},
  doi = {10.1007/s11063-012-9230-4}
}

@INCOLLECTION{MacKay:1998,
  author = {MacKay, David J. C.},
  title = {Introduction to {Gaussian} Processes},
  booktitle = {Neural Networks and Machine Learning},
  publisher = {Springer},
  year = {1998},
  editor = {Bishop, Christopher M.},
  pages = {133--166}
}

@ARTICLE{MacKay:1999,
  author = {MacKay, David J. C.},
  title = {Comparison of Approximate Methods for Handling Hyperparameters},
  journal = {Neural Computation},
  year = {1999},
  volume = {11},
  pages = {1035--1068}
}

@INCOLLECTION{Neal:2011,
  author = {Neal, Radford M.},
  title = {{MCMC} using {Hamiltonian} dynamics},
  booktitle = {Handbook of {Markov} Chain {Monte} {Carlo}},
  publisher = {Chapman and Hall/CRC},
  year = {2011},
  editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li
	Meng}
}

@ARTICLE{Neal:2003,
  author = {Neal, Radford M.},
  title = {Slice sampling},
  journal = {The Annals of Statistics},
  year = {2003},
  volume = {31},
  pages = {705--767},
  number = {3}
}

@TECHREPORT{Neal:1993,
  author = {Neal, Radford M.},
  title = {Probabilistic Inference Using {Markov} Chain {Monte} {Carlo} Methods},
  institution = {Department of Computer Science, University of Toronto},
  year = {1993}
}

@ARTICLE{Niessner:1983,
  author = {Niessner, H. and Reichert, K.},
  title = {On computing the inverse of a sparse matrix},
  journal = {International Journal for Numerical Methods in Engineering},
  year = {1983},
  volume = {19},
  pages = {1513--1526}
}

@ARTICLE{Parker:2011,
  author = {Parker, Albert and Fox, Colin},
  title = {Sampling {Gaussian} distributions in {Krylov} spaces with conjugate
	gradients},
  note = {under review}
}

@ARTICLE{Pearson:1901,
  author = {Pearson, Karl},
  title = {On lines and planes of closest fit to systems of points in space},
  journal = {Philosophical Magazine},
  year = {1901},
  volume = {2},
  pages = {559--572},
  number = {6}
}

@MISC{Petersen:2008,
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  title = {The Matrix Cookbook},
  month = oct,
  year = {2008},
  note = {Version 20081110},
  publisher = {Technical University of Denmark},
  url = {http://www2.imm.dtu.dk/pubdb/p.php?3274}
}

@PHDTHESIS{Puggioni:2009,
  author = {Puggioni, Gavino},
  title = {Using Data Augmentation and Stochastic Differential Equations in
	Spatio Temporal Modeling},
  school = {Duke University},
  year = {2009}
}

@ARTICLE{Quinonero-Candela:2005,
  author = {Qui\~nonero{-}Candela, Joaquin and Rasmussen, Carl Edward},
  title = {A Unifying View of Sparse Approximate {Gaussian} Process Regression},
  journal = {Journal of Machine Learning Research},
  year = {2005},
  volume = {6},
  pages = {1939--1959},
  month = dec
}

@BOOK{Rasmussen:2006,
  title = {Gaussian Processes for Machine Learning},
  publisher = {MIT Press},
  year = {2006},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.}
}

@ARTICLE{Rauch:1965,
  author = {Rauch, H. E. and Tung, F. and Striebel, C. T.},
  title = {Maximum likelihood estimates of linear dynamic systems},
  journal = {AIAA Journal},
  year = {1965},
  volume = {3},
  pages = {1445--1450},
  number = {8},
  owner = {jluttine},
  timestamp = {2013.04.19}
}

@ARTICLE{Rougier:2008,
  author = {Rougier, Jonathan},
  title = {Efficient Emulators for Multivariate Deterministic Functions},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2008},
  volume = {17},
  pages = {827--843},
  number = {4},
  month = {dec},
  abstract = {One of the challenges with emulating the response of a multivariate
	function to its inputs is the quantity of data that must be assimilated,
	which is the product of the number of model evaluations and the number
	of outputs. This article shows how even large calculations can be
	made tractable. It is already appreciated that gains can be made
	when the emulator residual covariance function is treated as separable
	in the model-inputs and model-outputs. Here, an additional simplification
	on the structure of the regressors in the emulator mean function
	allows very substantial further gains. The result is that it is now
	possible to emulate rapidly—on a desktop computer—models with hundreds
	of evaluations and hundreds of outputs. This is demonstrated through
	calculating costs in floating-point operations, and in an illustration.
	Even larger sets of outputs are possible if they have additional
	structure, for example, spatial-temporal.},
  doi = {10.1198/106186008X384032}
}

@BOOK{Saad:2003,
  title = {Iterative Methods for Sparse Linear Systems},
  year = {2003},
  author = {Saad, Yousef}
}

@ARTICLE{Schmidt:2008,
  author = {Schmidt, Mikkel N. and Laurberg, Hans},
  title = {Nonnegative matrix factorization with {Gaussian} process priors},
  journal = {Computational Intelligence and Neuroscience},
  year = {2008},
  volume = {2008},
  pages = {1--10},
  address = {New York, NY, United States},
  publisher = {Hindawi Publishing Corporation}
}

@TECHREPORT{Shewchuk:1994,
  author = {Shewchuk, Jonathan Richard},
  title = {An introduction to the conjugate gradient method without the agonizing
	pain},
  institution = {Carnegie Mellon University},
  year = {1994}
}

@BOOK{Shumway:2000,
  title = {Time Series Analysis and Its Applications},
  publisher = {Springer},
  year = {2000},
  author = {Robert H. Shumway and David S. Stoffer},
  owner = {jluttine},
  timestamp = {2013.04.19}
}

@ARTICLE{Sinnott:1984,
  author = {Sinnott, Roger W.},
  title = {Virtues of the Haversine},
  journal = {Sky and Telescope},
  year = {1984},
  volume = {68},
  pages = {159},
  number = {2}
}

@ARTICLE{Smith:1995,
  author = {Smith, S. P.},
  title = {Differentiation of the {Cholesky} Algorithm},
  journal = {Journal of Computational and Graphical Statistics},
  year = {1995},
  volume = {4},
  pages = {134--147},
  number = {2}
}

@ARTICLE{Stein:2005,
  author = {Stein, Michael L.},
  title = {Space-time covariance functions},
  journal = {Journal of the American Statistical Association},
  year = {2005},
  volume = {100},
  pages = {310--321},
  number = {469}
}

@BOOK{vonStorch:1999,
  title = {Statistical analysis in climate research},
  publisher = {Cambridge University Press},
  year = {1999},
  author = {von Storch, H. and Zwiers, W.},
  address = {Cambridge, UK}
}

@INPROCEEDINGS{Takahashi:1973,
  author = {Takahashi, K. and Fagan, J. and Chen, M.-S.},
  title = {Formation of a sparse bus impedance matrix and its application to
	short circuit study},
  booktitle = {Power Industry Computer Application Conference Proceedings},
  year = {1973}
}

@ARTICLE{Tipping:1999,
  author = {Tipping, Michael E. and Bishop, Christopher M.},
  title = {Probabilistic Principal Component Analysis},
  journal = {Journal of the Royal Statistical Society Series B},
  year = {1999},
  volume = {61},
  pages = {611--622},
  number = {3}
}

@MISC{Walsh:2004,
  author = {Walsh, B.},
  title = {Markov Chain Monte Carlo and Gibbs Sampling}
}

@INCOLLECTION{Wikle:2006,
  author = {Wikle, Christopher K. and Hooten, Mevin B.},
  title = {Hierarchical {Bayesian} spatio–temporal models for population spread},
  booktitle = {Hierarchical Modelling for the Environmental Sciences: Statistical
	Methods and Applications},
  publisher = {Oxford University Press},
  year = {2006},
  editor = {Clark, James S. and Gelfand, Alan},
  chapter = {8},
  pages = {145--169},
  abstract = {There is a long history in the ecological sciences concerning the
	development of mathematical models for describing the distribution
	of organisms over time and space. Although such models have sometimes
	been evaluated by comparison to observations, they have seldom been
	“fit” to data in a formal statistical sense. Thus, uncertainties
	regarding data, model, and parameters are not readily accounted for
	in such analyses. Alternatively, realistic statistical models for
	spatio–temporal processes in ecology often require that one estimate
	a very large number of parameters. This is typically not possible
	given the relatively limited amount of data collected over space
	and time. Critically, such a statistical model could be simplified
	(in terms of reducing the number of parameters) by accommodating
	the well known empirical and theoretical results concerning the process
	(e.g. partial differential equations (PDEs) or integro–difference
	equations (IDEs) for population spread). That is, the statistical
	model can make use of the well established mathematical models for
	the process. The Bayesian hierarchical paradigm for spatio–temporal
	models allows one to account for the aforementioned sources of uncertainty
	and yet still include such prior knowledge for the process, parameters
	and measurements. In this chapter, we provide an introduction to
	process-based hierarchical Bayesian spatio–temporal models for population
	spread, focusing primarily on PDE dynamics. We illustrate the concepts
	and demonstrate the methodology on the problem of predicting the
	Eurasian Collared-Dove invasion of North America.}
}

@ARTICLE{Wikle:2001,
  author = {Wikle, Christopher K. and Milliff, Ralph F. and Nychka, Doug and
	Berliner, L. Mark},
  title = {Spatio-Temporal Hierarchical {Bayesian} Modeling: Tropical Ocean
	Surface Winds},
  journal = {Journal of the American Statistical Association},
  year = {2001},
  volume = {96},
  pages = {382--397}
}

@INPROCEEDINGS{Wilson:2011,
  author = {Wilson, Andrew Gordon and Ghahramani, Zoubin},
  title = {Generalised {Wishart} Processes},
  booktitle = {Proceedings of the Proceedings of the Twenty-Seventh Conference Annual
	Conference on Uncertainty in Artificial Intelligence (UAI 2011)},
  year = {2011},
  pages = {736-744},
  address = {Corvallis, Oregon},
  publisher = {AUAI Press},
  abstract = {We introduce a new stochastic process called the generalised Wishart
	process (GWP). It is a collection of positive semi-definite random
	matrices indexed by any arbitrary input variable. We use this process
	as a prior over dynamic (e.g. time varying) covariance matrices.
	The GWP captures a diverse class of covariance dynamics, naturally
	handles missing data, scales nicely with dimension, has easily interpretable
	parameters, and can use input variables that include covariates other
	than time. We describe how to construct the GWP, introduce general
	procedures for inference and prediction, and show that it outperforms
	its main competitor, multivariate GARCH, even on financial data that
	especially suits GARCH.}
}

@ARTICLE{Zastavnyi:2011,
  author = {Zastavnyi, Viktor P. and Porcu, Emilio},
  title = {Characterization theorems for the {Gneiting} class of space-time
	covariances},
  journal = {Bernoulli},
  year = {2011},
  volume = {17},
  pages = {456--465},
  number = {1},
  abstract = {We characterize the Gneiting class of space–time covariance functions
	and give more relaxed conditions on the functions involved. We then
	show necessary conditions for the construction of compactly supported
	functions of the Gneiting type. These conditions are very general
	since they do not depend on the Euclidean norm.},
  doi = {10.3150/10-BEJ278}
}

@ARTICLE{Zhang:2007,
  author = {Zhang, Hao},
  title = {Maximum-likelihood estimation for multivariate spatial linear coregionalization
	models},
  journal = {Environmetrics},
  year = {2007},
  volume = {18},
  pages = {125--139},
  number = {2},
  doi = {10.1002/env.807}
}

@BOOK{NIPS:2002,
  title = {Advances in Neural Information Processing Systems 15},
  publisher = {MIT Press},
  year = {2003},
  editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 15}
}

@BOOK{NIPS:2009,
  title = {Advances in Neural Information Processing Systems 22},
  publisher = {MIT Press},
  year = {2009},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. K.
	I. and Culotta, A.},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 22}
}

@PROCEEDINGS{AISTATS:2003,
  year = {2003},
  editor = {Bishop, Christopher M. and Frey, Brendan J.},
  publisher = {Society for Artificial Intelligence and Statistics},
  booktitle = {Proceedings of the 9th International Workshop on Artificial Intelligence
	and Statistics (AISTATS'03)}
}

@PROCEEDINGS{ICML:2009,
  title = {Proceedings of the 26th International Conference on Machine Learning
	(ICML'09)},
  year = {2009},
  editor = {L\'{e}on Bottou and Michael Littman},
  address = {Montreal},
  publisher = {Omnipress},
  month = jun,
  booktitle = {Proceedings of the 26th International Conference on Machine Learning
	(ICML'09)}
}

@PROCEEDINGS{AISTATS:2005,
  year = {2005},
  editor = {Cowell, Robert G. and Ghahramani, Zoubin},
  publisher = {Society for Artificial Intelligence and Statistics},
  booktitle = {Proceedings of the 10th International Workshop on Artificial Intelligence
	and Statistics (AISTATS'05)}
}

@BOOK{NIPS:2001,
  title = {Advances in Neural Information Processing Systems 14},
  publisher = {MIT Press},
  year = {2002},
  editor = {Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 14}
}

@PROCEEDINGS{AISTATS:2009,
  year = {2009},
  editor = {van Dyk, David and Welling, Max},
  publisher = {Society for Artificial Intelligence and Statistics},
  booktitle = {Proceedings of the 12th International Workshop on Artificial Intelligence
	and Statistics (AISTATS'09)}
}

@BOOK{Finkenstadt:2007,
  title = {Statistical Methods for Spatio-Temporal Systems},
  publisher = {Chapman and Hall/CRC},
  year = {2007},
  editor = {Finkenst\"adt, B\"arbel and Held, Leonhard and Isham, Valerie},
  number = {107},
  series = {Monographs on Statistics and Applied Probability}
}

@BOOK{NIPS:2008,
  title = {Advances in Neural Information Processing Systems 21},
  publisher = {MIT Press},
  year = {2009},
  editor = {Koller, Daphne and Schuurmans, Dale and Bengio, Yoshua and Bottou,
	L\'eon},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 21}
}

@BOOK{NIPS:2010,
  title = {Advances in Neural Information Processing Systems 23},
  publisher = {MIT Press},
  year = {2010},
  editor = {Lafferty, J. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel,R.S.
	and Culotta, A.},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 23}
}

@BOOK{NIPS:2000,
  title = {Advances in Neural Information Processing Systems 13},
  publisher = {MIT Press},
  year = {2001},
  editor = {Leen, Todd K. and Dietterich Thomas G. and Tresp, Volker},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 13}
}

@PROCEEDINGS{UAI:2008,
  title = {Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence
	(UAI'2008)},
  year = {2008},
  editor = {McAllester, David A. and Myllym{\"a}ki, Petri},
  address = {Helsinki, Finland},
  publisher = {AUAI Press},
  month = jul,
  booktitle = {Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence
	(UAI'2008)}
}

@PROCEEDINGS{AISTATS:2007,
  year = {2007},
  editor = {Meila, Marina and Shen, Xiaotong},
  publisher = {Society for Artificial Intelligence and Statistics},
  booktitle = {Proceedings of the 11th International Workshop on Artificial Intelligence
	and Statistics (AISTATS'07)}
}

@BOOK{NIPS:2007,
  title = {Advances in Neural Information Processing Systems 20},
  publisher = {MIT Press},
  year = {2008},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S.},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 20}
}

@BOOK{NIPS:2004,
  title = {Advances in Neural Information Processing Systems 17},
  publisher = {MIT Press},
  year = {2005},
  editor = {Saul, Lawrence K. and Weiss, Yair and Bottou, L\´eon},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 17}
}

@BOOK{NIPS:2006,
  title = {Advances in Neural Information Processing Systems 19},
  publisher = {MIT Press},
  year = {2007},
  editor = {Sch\"olkopf, Bernhard and Platt, John and Hoffman, Thomas},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 19}
}

@BOOK{NIPS:2003,
  title = {Advances in Neural Information Processing Systems 16},
  publisher = {MIT Press},
  year = {2004},
  editor = {Thrun, Sebastian and Saul, Lawrence K. and Sch\"olkopf, Bernhard},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 16}
}

@BOOK{NIPS:1995,
  title = {Advances in Neural Information Processing Systems 8},
  publisher = {MIT Press},
  year = {1996},
  editor = {Touretzky, David S. and Mozer, Michael C. and Hasselmo, Michael E.},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 8}
}

@PROCEEDINGS{ESANN:2009,
  year = {2009},
  editor = {Verleysen, Michel},
  publisher = {d-side},
  booktitle = {Proceedings of the 17th European Symposium on Artificial Neural Networks
	(ESANN'2009)}
}

@BOOK{NIPS:2005,
  title = {Advances in Neural Information Processing Systems 18},
  publisher = {MIT Press},
  year = {2006},
  editor = {Weiss, Yair and Sch\"olkopf, Bernhard and Platt, John},
  address = {Cambridge, MA},
  booktitle = {Advances in Neural Information Processing Systems 18}
}

@INPROCEEDINGS{Agovic:2011,
  title = {Probabilistic Matrix Addition},
  abstract = {We introduce Probabilistic Matrix Addition (PMA) for modeling real-valued
	data matrices by simultaneously capturing covariance structure among
	rows and among columns. PMA additively combines two latent matrices
	drawn from two Gaussian Processes respectively over rows and columns.
	The resulting joint distribution over the observed matrix does not
	factorize over entries, rows, or columns, and can thus capture intricate
	dependencies in the matrix. Exact inference in PMA is possible, but
	involves inversion of large matrices, and can be computationally
	prohibitive. Efficient approximate inference is possible due to the
	sparse dependency structure among latent variables. We propose two
	families of approximate inference algorithms for PMA based on Gibbs
	sampling and MAP inference. We demonstrate the effectiveness of PMA
	for missing value prediction and multi-label classification problems.}
}

@ARTICLE{Barry:1999,
  title = {{Monte} {Carlo} estimates of the log determinant of large sparse
	matrices}
}

@ARTICLE{Chen:2011,
  title = {Computing $f(A)b$ via least squares polynomial approximations}
}

@TECHREPORT{Huang:2009,
  title = {On the Validity of Covariance and Variogram Functions on the Sphere}
}

@ARTICLE{Kaplan:1997,
}

@INPROCEEDINGS{Lazaro-Gredilla:2011,
  title = {Variational Heteroscedastic {Gaussian} Process Regression},
  abstract = {Standard Gaussian processes (GPs) model observations’ noise as constant
	throughout input space. This is often a too restrictive assumption,
	but one that is needed for GP inference to be tractable. In this
	work
	
	we present a non-standard variational approximation that allows accurate
	inference in heteroscedastic GPs (i.e., under input-dependent noise
	conditions). Computational cost is roughly twice that of the standard
	GP, and also scales as O($n^3$). Accuracy is verified by comparing
	with the golden standard MCMC and its effectiveness is illustrated
	on several synthetic and real datasets of diverse characteristics.
	An application to volatility forecasting is also considered.}
}

@ARTICLE{Lopes:2011,
  title = {Generalized Spatial Dynamic Factor Models}
}

@ELECTRONIC{McCourt:2008,
  title = {A Stochastic Simulation for Approximating the log-Determinant of
	a Symmetric Positive Definite Matrix}
}

@ARTICLE{Paciorek:2007,
  title = {{Bayesian} Smoothing with {Gaussian} Processes Using {Fourier} Basis
	Functions in the {spectralGP} Package}
}

@TECHREPORT{WHOWHATWHERE,
}

@ARTICLE{Zhang:2008,
  title = {Log-det approximation based on uniformly distributed seeds and its
	application to {Gaussian} process regression}
}

@ARTICLE{,
}

@PROCEEDINGS{AISTATS:2011,
  title = {Proceedings of the International Conference on Artificial Intelligence
	and Statistics (AISTATS)},
  year = {2011}
}

@ARTICLE{Karspeck:2011,
  title = {{Bayesian} modelling and ensemble reconstruction of mid-scale spatial
	variability in {North} {Atlantic} sea-surface temperatures for 1850--2008},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  year = {2011},
  abstract = {Existing historical records of sea-surface temperature extending back
	to the mid-1800s are a valuable source of information about climate
	variability on interannual and decadal time-scales. However, the
	temporal and spatial irregularity of these data make them difficult
	to use in climate research, where gridded and complete data fields
	are expected for both statistical analysis and forcing numerical
	models.
	
	
	Infilling methods based on constraining the solution to the linear
	space spanned by the leading eigenvectors of the global-scale covariance,
	otherwise known as reduced-space methods, have proven very successful
	in creating gridded estimates of sea-surface temperature. These methods
	are especially useful for infilling the vast regions of unobserved
	ocean typical of the earliest segments of the data record. Regional
	variability, on the other hand, is not well represented by these
	methods, especially in data-poor regions. Here we present a method
	for augmenting the established large-scale reconstruction methods
	with a statistical model of the mid-scale variability. Using high
	quality sea-surface temperature data from the last 30 years including
	satellite-derived records, we specify a spatially non-stationary,
	anisotropic covariance model for the mid-scale sea-surface temperature
	variability. With the parameters of the covariance model estimated
	from the modern record, historical observations are used for conditioning
	the posterior distribution. Specifically, we form the expected value
	and correlated uncertainty of the mid-scales as well as generating
	samples from the posterior.
	
	
	While this work focuses on a limited domain in the midlatitude North
	Atlantic Ocean, the method employed here can be extended to global
	reconstructions.},
  doi = {10.1002/qj.900}
}

@ARTICLE{Sun:2011,
  title = {Geostatistics for Large Datasets},
  year = {2011},
  abstract = {We review various approaches for the geostatistical analysis of large
	datasets. First, we consider covariance structures that yield computational
	simplifications in geostatistics and briefly discuss how to test
	the suitability of such structures. Second, we describe the use of
	covariance tapering for both estimation and kriging purposes. Third,
	we consider likelihood approximations in both spatial and spectral
	domains. Fourth, we explore methods based on latent processes, such
	as Gaussian predictive processes and fixed rank kriging. Fifth, we
	describe methods based on Gaussian Markov random field approximations.
	Finally, we discuss multivariate extensions and open problems in
	this area.}
}

@ARTICLE{Konukoglu:2010,
  title = {Image Guided Personalization of Reaction-Diffusion Type Tumor Growth
	Models Using Modified Anisotropic {Eikonal} Equations},
  journal = {IEEE Transactions on Medical Imaging},
  year = {2010},
  volume = {29},
  pages = {77--95},
  abstract = {Reaction-diffusion based tumor growth models have been widely used
	in the literature for modeling the growth of brain gliomas. Lately,
	recent models have started integrating medical images in their formulation.
	Including different tissue types, geometry of the brain and the directions
	of white matter fiber tracts improved the spatial accuracy of reaction-diffusion
	models. The adaptation of the general model to the specific patient
	cases on the other hand has not been studied thoroughly yet. In this
	paper, we address this adaptation. We propose a parameter estimation
	method for reaction-diffusion tumor growth models using time series
	of medical images. This method estimates the patient specific parameters
	of the model using the images of the patient taken at successive
	time instances. The proposed method formulates the evolution of the
	tumor delineation visible in the images based on the reaction-diffusion
	dynamics; therefore, it remains consistent with the information available.
	We perform thorough analysis of the method using synthetic tumors
	and show important couplings between parameters of the reaction-diffusion
	model. We show that several parameters can be uniquely identified
	in the case of fixing one parameter, namely the proliferation rate
	of tumor cells. Moreover, regardless of the value the proliferation
	rate is fixed to, the speed of growth of the tumor can be estimated
	in terms of the model parameters with accuracy. We also show that
	using the model-based speed, we can simulate the evolution of the
	tumor for the specific patient case. Finally, we apply our method
	to two real cases and show promising preliminary results.},
  doi = {10.1109/TMI.2009.2026413}
}

@PROCEEDINGS{ICA:2009,
  year = {2009},
  publisher = {Springer-Verlag},
  booktitle = {Proceedings of the 8th International Conference on Independent Component
	Analysis and Signal Separation (ICA'2009)}
}

@PROCEEDINGS{ICIP:2007,
  year = {2007},
  publisher = {IEEE},
  month = sep,
  booktitle = {Proceedings of the International Conference on Image Processing (ICIP'2007)}
}

@PROCEEDINGS{ICML:2006,
  year = {2006},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning
	(ICML'06)}
}

@PROCEEDINGS{ICANN:1999,
  year = {1999},
  booktitle = {Proceedings of the 9th International Conference on Artificial Neural
	Networks (ICANN'99)}
}

@ARTICLE{Hasselmann:1976,
  title = {Stochastic climate models},
  year = {1976}
}

