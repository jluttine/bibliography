% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{Archambeau:2006,
  Title                    = {Robust probabilistic projections},
  Author                   = {Archambeau, C\'{e}dric and Delannay, Nicolas and Verleysen, Michel},
  Pages                    = {33--40},

  Crossref                 = {ICML:2006}
}

@InCollection{Attias:2000,
  Title                    = {A Variational {Bayesian} Framework for Graphical Models},
  Author                   = {Attias, Hagai},

  Crossref                 = {NIPS:1999}
}

@InCollection{Barber:2007,
  Title                    = {Unified Inference for Variational {Bayesian} Linear {Gaussian} State-Space Models},
  Author                   = {Barber, David and Chiappa, Silvia},

  Crossref                 = {NIPS:2006}
}

@InProceedings{Bishop:1999,
  Title                    = {Variational principal components},
  Author                   = {Bishop, Christopher M.},
  Pages                    = {509--514},

  Abstract                 = {testi vaan},
  Crossref                 = {ICANN:1999}
}

@InCollection{Bishop:2002,
  Title                    = {{VIBES:} A Variational Inference Engine for {Bayesian} Networks},
  Author                   = {Bishop, Christopher M. and Spiegelhalter, David and Winn, John},

  Crossref                 = {NIPS:2002},
  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@InCollection{Bonilla:2008,
  Title                    = {Multi-task {Gaussian} Process Regression},
  Author                   = {Bonilla, Edwin V. and Chai, Kian Ming A. and Williams, Christopher K. I.},

  Crossref                 = {NIPS:2007}
}

@InCollection{Boyle:2005,
  Title                    = {Dependent {Gaussian} Processes},
  Author                   = {Boyle, Phillip and Frean, Marcus},

  Crossref                 = {NIPS:2004}
}

@InCollection{Chai:2009,
  Title                    = {Multi-task {Gaussian} Process Learning of Robot Inverse Dynamics},
  Author                   = {Chai, Kian Ming A. and Williams, Christopher K. I. and Klanke, Stefan and Vijayakumar, Sethu},

  Crossref                 = {NIPS:2008},
  Owner                    = {jluttine},
  Timestamp                = {2014.10.22}
}

@InCollection{Collins:2002,
  Title                    = {A generalization of principal components analysis to the exponential family},
  Author                   = {M. Collins and S. Dasgupta and R. Schapire},
  Pages                    = {617--624},

  Crossref                 = {NIPS:2001}
}

@InCollection{Ghahramani:1999,
  Title                    = {Learning nonlinear dynamical systems using an {EM} algorithm},
  Author                   = {Ghahramani, Zoubin and Roweis, Sam T.},
  Pages                    = {431--437},

  Crossref                 = {NIPS:1998}
}

@InCollection{Griffiths:2006,
  Title                    = {Infinite Latent Feature Models and the {Indian} Buffet Process},
  Author                   = {Griffiths, Thomas L. and Ghahramani, Zoubin},
  Pages                    = {475--482},

  Crossref                 = {NIPS:2005}
}

@InCollection{Hensman:2012,
  Title                    = {Fast Variational Inference in the Conjugate Exponential Family},
  Author                   = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},

  Crossref                 = {NIPS:2012},
  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@InCollection{Honkela:2005,
  Title                    = {Unsupervised Variational {B}ayesian Learning of Nonlinear Models},
  Author                   = {A. Honkela and H. Valpola},
  Pages                    = {593--600},

  Crossref                 = {NIPS:2004}
}

@InCollection{Lazaro-Gredilla:2009,
  Title                    = {Inter-domain {Gaussian} Processes for Sparse Inference using Inducing Features},
  Author                   = {L\'{a}zaro-Gredilla, Miguel and Figueiras-Vidal, An\'{i}bal R.},

  Abstract                 = {We present a general inference framework for inter-domain Gaussian Processes (GPs) and focus on its usefulness to build sparse GP models. The state-of-the-art sparse GP model introduced by Snelson and Ghahramani in [1] relies on finding a small, representative pseudo data set of m elements (from the same domain as the n available data elements) which is able to explain existing data well, and then uses it to perform inference. This reduces inference and model selection computation time from O(n3) to O(m2n), where m n. Inter-domain GPs can be used to find a (possibly more compact) representative set of features lying in a different domain, at the same computational cost. Being able to specify a different domain for the representative features allows to incorporate prior knowledge about relevant characteristics of data and detaches the functional form of the covariance and basis functions. We will show how previously existing models fit into this framework and will use it to develop two new sparse GP models. Tests on large, representative regression data sets suggest that significant improvement can be achieved, while retaining computational efficiency.},
  Crossref                 = {NIPS:2009}
}

@InProceedings{Lakshminarayanan:2011,
  Title                    = {Robust {Bayesian} Matrix Factorisation},
  Author                   = {Lakshminarayanan, Balaji and Bouchard, Guillaume and Archambeau, Cedric},

  Crossref                 = {AISTATS:2011}
}

@InCollection{Lawrence:2004,
  Title                    = {Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data},
  Author                   = {Lawrence, Neil D.},
  Pages                    = {329--336},

  Crossref                 = {NIPS:2003}
}

@InProceedings{Lawrence:2002,
  Title                    = {Fast sparse {Gaussian} process methods: the informative vector machine},
  Author                   = {Lawrence, Neil D. and Seeger, Matthias and Herbrich, Ralf},

  Crossref                 = {NIPS:2002}
}

@InProceedings{Lawrence:2009,
  Title                    = {Non-linear Matrix Factorization with {Gaussian} Processes},
  Author                   = {Neil D. Lawrence and Raquel Urtasun},
  Pages                    = {601--608},

  Crossref                 = {ICML:2009}
}

@InCollection{Luttinen:2009:nips,
  Title                    = {Variational {Gaussian}-process factor analysis for modeling spatio-temporal data},
  Author                   = {Luttinen, Jaakko and Ilin, Alexander},

  Crossref                 = {NIPS:2009},
  File                     = {:Luttinen\:2009\:nips.pdf:PDF}
}

@InProceedings{Luttinen:2009:ica,
  Title                    = {Bayesian Robust {PCA} for Incomplete Data},
  Author                   = {Luttinen, Jaakko and Ilin, Alexander and Karhunen, Juha},
  Pages                    = {66--73},

  Crossref                 = {ICA:2009},
  Doi                      = {10.1007/978-3-642-00599-2_9},
  File                     = {:Luttinen\:2009\:ica.pdf:PDF}
}

@InProceedings{Luttinen:2009:esann,
  Title                    = {Transformations for Variational Factor Analysis to Speed up Learning},
  Author                   = {Luttinen, Jaakko and Ilin, Alexander and Raiko, Tapani},
  Pages                    = {77--82},

  Crossref                 = {ESANN:2009},
  File                     = {:/home/jluttine/bibliography/pdf/Luttinen\:2009\:esann.pdf:PDF}
}

@InCollection{Minka:2001,
  Title                    = {Automatic Choice of Dimensionality for {PCA}},
  Author                   = {Minka, Thomas P.},
  Pages                    = {598-604},

  Crossref                 = {NIPS:2000}
}

@InCollection{Mohamed:2008,
  Title                    = {Bayesian Exponential Family {PCA}},
  Author                   = {Mohamed, Shakir and Heller, Katherine and Ghahramani, Zoubin},
  Pages                    = {1089--1096},

  Crossref                 = {NIPS:2008}
}

@InCollection{Murray:2010,
  Title                    = {Slice sampling covariance hyperparameters of latent {Gaussian} models},
  Author                   = {Murray, Iain and Adams, Ryan Prescott},

  Crossref                 = {NIPS:2010}
}

@InCollection{Orbanz:2009,
  Title                    = {Construction of Nonparametric {Bayesian} Models from Parametric {Bayes} Equations},
  Author                   = {Orbanz, Peter},

  Crossref                 = {NIPS:2009}
}

@InProceedings{Pang:2007,
  Title                    = {Monocular Tracking {3D} People By {Gaussian} Process Spatio-Temporal Variable Model},
  Author                   = {Pang, Junbiao and Qing, Laiyun and Huang, Qingming and Jiang, Shuqiang and Gao, Wen},
  Pages                    = {41--44},

  Crossref                 = {ICIP:2007}
}

@InCollection{Qi:2007,
  Title                    = {Parameter Expanded Variational {Bayesian} Methods},
  Author                   = {Qi, Yuan (Alan) and Jaakkola, Tommi S.},
  Pages                    = {1097--1104},

  Crossref                 = {NIPS:2006}
}

@InProceedings{Schmidt:2009,
  Title                    = {Function factorization using warped {Gaussian} processes},
  Author                   = {Schmidt, Mikkel N.},
  Pages                    = {921--928},

  Crossref                 = {ICML:2009}
}

@InProceedings{Seeger:2003,
  Title                    = {Fast Forward Selection to Speed Up Sparse Gaussian Process Regression},
  Author                   = {Seeger, Matthias and Williams, Christopher K. I. and Lawrence, Neil D.},
  Pages                    = {205--213},

  Crossref                 = {AISTATS:2003}
}

@InProceedings{Smola:2001,
  Title                    = {Sparse greedy {Gaussian} process regression},
  Author                   = {Smola, Alex J. and Bartlett, Peter},

  Crossref                 = {NIPS:2000}
}

@InCollection{Snelson:2006,
  Title                    = {Sparse Gaussian Processes using Pseudo-inputs},
  Author                   = {Snelson, Edward and Ghahramani, Zoubin},
  Pages                    = {1257--1264},

  Crossref                 = {NIPS:2005}
}

@InProceedings{Snelson:2007,
  Title                    = {Local and global sparse {Gaussian} process approximations},
  Author                   = {Snelson, Edward and Ghahramani, Zoubin},

  Crossref                 = {AISTATS:2007}
}

@InProceedings{Teh:2005,
  Title                    = {Semiparametric Latent Factor Models},
  Author                   = {Teh, Yee Whye and Seeger, Matthias and Jordan, Michael I.},
  Pages                    = {333--340},

  Crossref                 = {AISTATS:2005}
}

@InProceedings{Titsias:2009,
  Title                    = {Variational Learning of Inducing Variables in Sparse {Gaussian} Processes},
  Author                   = {Titsias, Michalis K.},
  Pages                    = {567--574},

  Crossref                 = {AISTATS:2009}
}

@InCollection{Tresp:2001,
  Title                    = {Mixtures of {Gaussian} Processes},
  Author                   = {Tresp, Volker},
  Pages                    = {654--660},

  Crossref                 = {NIPS:2000}
}

@InProceedings{Vanhatalo:2008,
  Title                    = {Modelling local and global phenomena with sparse {Gaussian} processes},
  Author                   = {Vanhatalo, Jarno and Vehtari, Aki},
  Pages                    = {571--578},

  Crossref                 = {UAI:2008}
}

@InProceedings{Williams:1996,
  Title                    = {Gaussian Processes for Regression},
  Author                   = {Williams, Christopher K. I. and Rasmussen, Carl Edward},

  Crossref                 = {NIPS:1995}
}

@InProceedings{Williams:2001,
  Title                    = {Using the {Nystr\"om} method to speed up kernel machines},
  Author                   = {Williams, Christopher K. I. and Seeger, Matthias},

  Crossref                 = {NIPS:2000}
}

@InCollection{Wright:2009,
  Title                    = {Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices by Convex Optimization},
  Author                   = {Wright, John and Peng, Yigang and Ma, Yi and Ganesh, Arvind and Rao, Shankar},

  Crossref                 = {NIPS:2009}
}

@InCollection{Yu:2009,
  Title                    = {Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity},
  Author                   = {Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu, Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
  Pages                    = {1881--1888},

  Crossref                 = {NIPS:2008}
}

@InProceedings{Smidl:2003,
  Title                    = {Fast variational {PCA} for functional analysis of dynamic image sequences},
  Author                   = {\vSm\´idl, V\´aclav and A. Quinn},
  Booktitle                = {Proceedings of the 3rd International Symposium on Image and Signal Processing and Analysis (ISPA'2003)},
  Year                     = {2003},
  Month                    = {September},
  Pages                    = {555--560},
  Volume                   = {1}
}

@Book{Banerjee:2004,
  Title                    = {Hierarchical Modeling and Analysis for Spatial Data},
  Author                   = {Banerjee, Sudipto and Carlin, Bradley P. and Gelfand, Alan E.},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {2004},
  Series                   = {Monographs on Statistics and Applied Probability}
}

@Book{Bar-Shalom:2001,
  Title                    = {Estimation with Applications to Tracking and Navigation},
  Author                   = {Bar-Shalom, Y and Li, X Rong and Kirubarajan, T},
  Publisher                = {Wiley-Interscience},
  Year                     = {2001}
}

@PhdThesis{Beal:2003,
  Title                    = {Variational algorithms for approximate {Bayesian} inference},
  Author                   = {Beal, Matthew J.},
  School                   = {Gatsby Computational Neuroscience Unit, University College London},
  Year                     = {2003}
}

@Article{Beal:2003b,
  Title                    = {The Variational {Bayesian} {EM} Algorithm for Incomplete Data: with Application to Scoring Graphical Model Structures},
  Author                   = {Beal, M. J. and Ghahramani, Z.},
  Journal                  = {Bayesian Statistics},
  Year                     = {2003},
  Pages                    = {453--464},
  Volume                   = {7},

  Owner                    = {jluttine},
  Timestamp                = {2013.04.19}
}

@Book{Bishop:2006,
  Title                    = {Pattern Recognition and Machine Learning},
  Author                   = {Bishop, Christopher M.},
  Publisher                = {Springer},
  Year                     = {2006},

  Address                  = {New York},
  Edition                  = {2nd},
  Series                   = {Information Science and Statistics}
}

@Book{Bottomley:1990,
  Title                    = {Global Ocean Surface Temperature Atlas},
  Author                   = {Bottomley, M. and Folland, C. K. and Hsiung, J. and Newell, R. E. and Parker, D. E.},
  Publisher                = {Her Majesty's Stn. Off.},
  Year                     = {1990},

  Address                  = {Norwich, England}
}

@Article{Calder:2007,
  Title                    = {Dynamic factor process convolution models for multivariate space-time data with application to air quality assessment},
  Author                   = {Calder, Catherine A.},
  Journal                  = {Environmental and Ecological Statistics},
  Year                     = {2007},
  Number                   = {3},
  Pages                    = {229--247},
  Volume                   = {14}
}

@Article{Candes:2011,
  Title                    = {Robust Principal Component Analysis?},
  Author                   = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  Journal                  = {Journal of the ACM},
  Year                     = {2011},
  Pages                    = {37},
  Volume                   = {58}
}

@InProceedings{Chandrasekaran:2009,
  Title                    = {Sparse and Low-Rank Matrix Decomposition},
  Author                   = {Chandrasekaran, Venkat and Sanghavi, Sujay and Parrilo, Pablo A. and Willsky, Alan S.},
  Booktitle                = {IFAC Symposium on System Identification},
  Year                     = {2009}
}

@PhdThesis{Charles:2009,
  Title                    = {Stochastic Differential Equations: Applications in Transport modelling in shallow waters},
  Author                   = {Charles, Wilson Mahera},
  School                   = {Lappeenranta University of Technology},
  Year                     = {2009}
}

@Article{Clark:2005,
  Title                    = {Why environmental scientists are becoming {Bayesians}},
  Author                   = {Clark, James S.},
  Journal                  = {Ecology Letters},
  Year                     = {2005},
  Pages                    = {2--14},
  Volume                   = {8}
}

@Book{Cressie:1993,
  Title                    = {Statistics for Spatial Data},
  Author                   = {Cressie, Noel},
  Publisher                = {Wiley},
  Year                     = {1993},

  Address                  = {New York},
  Series                   = {Wiley Series in Probability and Mathematical Statistics}
}

@Article{Cressie:1999,
  Title                    = {Classes of Nonseparable, Spatio-Temporal Stationary Covariance Functions},
  Author                   = {Cressie, Noel and Huang, Hsin-Cheng},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1999},

  Month                    = dec,
  Number                   = {448},
  Pages                    = {1330--1340},
  Volume                   = {94}
}

@Article{Csato:2002,
  Title                    = {Sparse online {Gaussian} processes},
  Author                   = {Csató, Lehel and Opper, Manfred},
  Journal                  = {Neural Computation},
  Year                     = {2002},
  Number                   = {3},
  Pages                    = {641--668},
  Volume                   = {14}
}

@Article{DeCesare:2001,
  Title                    = {Estimating and modeling space-time correlation structures},
  Author                   = {De Cesare, L. and Myers, D. E. and Posa, D.},
  Journal                  = {Statistics \& Probability Letters},
  Year                     = {2001},
  Pages                    = {9--14},
  Volume                   = {51}
}

@Article{DeIaco:2001,
  Title                    = {Space-time analysis using a general product-sum model},
  Author                   = {De Iaco, S. and Myers, D. E. and Posa, D.},
  Journal                  = {Statistics \& Probability Letters},
  Year                     = {2001},
  Pages                    = {21--28},
  Volume                   = {52}
}

@Article{Ding:2011,
  Title                    = {Bayesian Robust Principal Component Analysis},
  Author                   = {Ding, Xinghao and He, Lihan and Carin, Lawrence},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2011},
  Number                   = {12},
  Pages                    = {3419--3430},
  Volume                   = {20}
}

@Misc{Doucet:2010,
  Title                    = {A Note on Efficient Conditional Simulation of {Gaussian} Distributions},

  Author                   = {Doucet, Arnaud}
}

@Article{Duan:2009,
  Title                    = {Modeling Space-Time Data Using Stochastic Differential Equations},
  Author                   = {Duan, Jason A. and Gelfand, Alan E. and Sirmans, C. F.},
  Journal                  = {Bayesian Analysis},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {733--758},
  Volume                   = {4},

  Abstract                 = {This paper demonstrates the use and value of stochastic differential equations for modeling space-time data in two common settings. The first consists of point-referenced or geostatistical data where observations are collected at fixed locations and times. The second considers random point pattern data where the emergence of locations and times is random. For both cases, we employ stochastic differential equations to describe a latent process within a hierarchical model for the data. The intent is to view this latent process mechanistically and endow it with appropriate simple features and interpretable parameters. A motivating problem for the second setting is to model urban development through observed locations and times of new home construction; this gives rise to a space-time point pattern. We show that a spatio-temporal Cox process whose intensity is driven by a stochastic logistic equation is a viable mechanistic model that affords meaningful interpretation for the results of statistical inference. Other applications of stochastic logistic differential equations with space-time varying parameters include modeling population growth and product diffusion, which motivate our first, point-referenced data application. We propose a method to discretize both time and space in order to fit the model. We demonstrate the inference for the geostatistical model through a simulated dataset. Then, we fit the Cox process model to a real dataset taken from the greater Dallas metropolitan area.},
  Doi                      = {10.1214/09-BA427}
}

@Article{vanDyk:2001,
  Title                    = {The Art of Data Augmentation},
  Author                   = {van Dyk, David A. and Meng, Xiao-Li},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {1--50},
  Volume                   = {10}
}

@Article{Eubank:2002,
  Title                    = {The Equivalence Between the {Cholesky} Decomposition and the {Kalman} Filter},
  Author                   = {Eubank, R. L. and Wang, Suojin},
  Journal                  = {The American Statistician},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {39--43},
  Volume                   = {56},

  Owner                    = {jluttine},
  Timestamp                = {2013.04.19}
}

@Article{Faul:2007,
  Title                    = {Gaussian process modeling of {EEG} for the detection of neonatal seizures},
  Author                   = {Faul, Stephen and Gregorcic, Gregor and Boylan, Geraldine and Marnane, William and Lightbody, Gordon and Connolly, Sean},
  Journal                  = {IEEE Transactions on Biomedical Engineering},
  Year                     = {2007},
  Number                   = {12},
  Pages                    = {2151--2162},
  Volume                   = {54},

  Doi                      = {10.1109/TBME.2007.895745},
  Publisher                = {IEEE}
}

@Article{Fox:2012,
  Title                    = {A tutorial on variational {Bayesian} inference},
  Author                   = {Fox, Charles W. and Roberts, Stephen J.},
  Journal                  = {Artificial Intelligence Review},
  Year                     = {2012},

  Month                    = {jun},
  Number                   = {2},
  Pages                    = {85--95},
  Volume                   = {38},

  Abstract                 = {This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
  Doi                      = {10.1007/s10462-011-9236-8}
}

@Article{Fritz:2009,
  Title                    = {Application of {FFT}-based Algorithms for Large-Scale Universal Kriging Problems},
  Author                   = {Fritz, J. and Neuweiler, I. and Nowak, W.},
  Journal                  = {Mathematical Geosciences},
  Year                     = {2009},
  Number                   = {5},
  Pages                    = {509--533},
  Volume                   = {41},

  Abstract                 = {Looking at kriging problems with huge numbers of estimation points and measurements, computational power and storage capacities often pose heavy limitations to the maximum manageable problem size. In the past, a list of FFT-based algorithms for matrix operations have been developed. They allow extremely fast convolution, superposition and inversion of covariance matrices under certain conditions. If adequately used in kriging problems, these algorithms lead to drastic speedup and reductions in storage requirements without changing the kriging estimator. However, they require second-order stationary covariance functions, estimation on regular grids, and the measurements must also form a regular grid. In this study, we show how to alleviate these rather heavy and many times unrealistic restrictions. Stationarity can be generalized to intrinsicity and beyond, if decomposing kriging problems into the sum of a stationary problem and a formally decoupled regression task. We use universal kriging, because it covers arbitrary forms of unknown drift and all cases of generalized covariance functions. Even more general, we use an extension to uncertain rather than unknown drift coefficients. The sampling locations may now be irregular, but must form a subset of the estimation grid. Finally, we present asymptotically exact but fast approximations to the estimation variance and point out application to conditional simulation, cokriging and sequential kriging. The drastic gain in computational and storage efficiency is demonstrated in test cases. Especially high-resolution and data-rich fields such as rainfall interpolation from radar measurements or seismic or other geophysical inversion can benefit from these improvements.},
  Doi                      = {10.1007/s11004-009-9220-x}
}

@Article{Fuentes:2006,
  Title                    = {Testing for separability of spatial-temporal covariance functions},
  Author                   = {Fuentes, Montserrat},
  Journal                  = {Journal of Statistical Planning and Inference},
  Year                     = {2006},
  Pages                    = {447--466},
  Volume                   = {136}
}

@Article{Furrer:2006,
  Title                    = {Covariance Tapering for Interpolation of Large Spatial Datasets},
  Author                   = {Furrer, Reinhard and Genton, Marc G. and Nychka, Douglas},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2006},
  Pages                    = {502--523},
  Volume                   = {15}
}

@Article{Gao:2008,
  Title                    = {Robust {L1} Principal Component Analysis and Its {Bayesian} variational inference},
  Author                   = {Gao, Junbin},
  Journal                  = {Neural computation},
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {555--572},
  Volume                   = {20}
}

@Book{Gelman:2003,
  Title                    = {Bayesian Data Analysis},
  Author                   = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {2003},

  Address                  = {Florida},
  Edition                  = {2nd},
  Series                   = {Texts in Statistical Science}
}

@Article{Geman:1984,
  Title                    = {Stochastic Relaxation, {Gibbs} Distributions, and the {Bayesian} Restoration of Images},
  Author                   = {Geman, Stuat and Geman, D.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {1984},
  Number                   = {6},
  Pages                    = {721--741},
  Volume                   = {6},

  Doi                      = {10.1109/TPAMI.1984.4767596}
}

@Article{Genton:2007,
  Title                    = {Separable approximations of space-time covariance matrices},
  Author                   = {Genton, Marc G.},
  Journal                  = {Environmetrics},
  Year                     = {2007},
  Pages                    = {681--695},
  Volume                   = {18}
}

@Article{Ghahramani:1998:sssm,
  Title                    = {Variational Learning for Switching State-Space Models},
  Author                   = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  Journal                  = {Neural Computation},
  Year                     = {1998},
  Pages                    = {963--996},
  Volume                   = {12},

  Owner                    = {jluttine},
  Timestamp                = {2014.09.11}
}

@Article{Girolami:2008,
  Title                    = {{Bayesian} inference for differential equations},
  Author                   = {Girolami, Mark},
  Journal                  = {Theoretical Computer Science},
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {4--16},
  Volume                   = {408},

  Abstract                 = {Nonlinear dynamic systems such as biochemical pathways can be represented in abstract form using a number of modelling formalisms. In particular differential equations provide a highly expressive mathematical framework with which to model dynamic systems, and a very natural way to model the dynamics of a biochemical pathway in a deterministic manner is through the use of nonlinear ordinary or time delay differential equations. However if, for example, we consider a biochemical pathway the constituent chemical species and hence the pathway structure are seldom fully characterised. In addition it is often impossible to obtain values of the rates of activation or decay which form the free parameters of the mathematical model. The system model in many cases is therefore not fully characterised either in terms of structure or the values which parameters take. This uncertainty must be accounted for in a systematic manner when the model is used in simulation or predictive mode to safeguard against reaching conclusions about system characteristics that are unwarranted, or in making predictions that are unjustifiably optimistic given the uncertainty about the model. The Bayesian inferential methodology provides a coherent framework with which to characterise and propagate uncertainty in such mechanistic models and this paper provides an introduction to Bayesian methodology as applied to system models represented as differential equations.},
  Doi                      = {10.1016/j.tcs.2008.07.005}
}

@Article{Girolami:2011,
  Title                    = {{Riemann} manifold {Langevin} and {Hamiltonian} {Monte} {Carlo} methods},
  Author                   = {Girolami, Mark and Calderhead, Ben},
  Journal                  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {123--214},
  Volume                   = {73},

  Doi                      = {10.1111/j.1467-9868.2010.00765.x}
}

@Article{Gneiting:2002,
  Title                    = {Compactly Supported Correlation Functions},
  Author                   = {Gneiting, Tilmann},
  Journal                  = {Journal of Multivariate Analysis},
  Year                     = {2002},
  Pages                    = {493--508},
  Volume                   = {83},

  Doi                      = {10.1006/jmva.2001.2056}
}

@Article{Gneiting:2002a,
  Title                    = {Nonseparable, Stationary Covariance Functions for Space-Time Data},
  Author                   = {Gneiting, Tilmann},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2002},
  Number                   = {458},
  Pages                    = {590--600},
  Volume                   = {97}
}

@Article{Goulard:1992,
  Title                    = {Linear Coregionalization Model: Tools for Estimation and Choice of Cross-Variogram Matrix},
  Author                   = {Goulard, M. and Voltz, M.},
  Journal                  = {Mathematical Geology},
  Year                     = {1992},
  Number                   = {3},
  Pages                    = {269--286},
  Volume                   = {24},

  Owner                    = {jluttine},
  Timestamp                = {2014.10.22}
}

@Article{Haario:2006,
  Title                    = {{DRAM:} Efficient adaptive {MCMC}},
  Author                   = {Haario, Heikki and Laine, Marko and Mira, Antonietta and Saksman, Eero},
  Journal                  = {Statistics and Computing},
  Year                     = {2006},
  Pages                    = {339--354},
  Volume                   = {16},

  Doi                      = {10.1007/s11222-006-9438-0}
}

@Article{Hastings:1970,
  Title                    = {Monte Carlo sampling methods using Markov cchain and their applications},
  Author                   = {Hastings, W. K.},
  Journal                  = {Biometrika},
  Year                     = {1970},
  Number                   = {1},
  Pages                    = {97--109},
  Volume                   = {57},

  Doi                      = {10.1093/biomet/57.1.97}
}

@InProceedings{Hensman:2013,
  Title                    = {Gaussian Processes for Big Data},
  Author                   = {Hensman, James and Fusi, Nicolo and Lawrence, Neil},
  Booktitle                = {Proceedings of the Twenty-Ninth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-13)},
  Year                     = {2013},
  Pages                    = {282--290},
  Publisher                = {AUAI Press},

  Abstract                 = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
  Added-at                 = {2013-09-30T12:05:23.000+0200},
  Biburl                   = {http://www.bibsonomy.org/bibtex/242ec2a9ea2dc7238a0ca148152203ef1/jluttine},
  Description              = {Gaussian Processes for Big Data},
  File                     = {:Hensman\:2013.pdf:PDF},
  Interhash                = {1dd1e6b3a097a6e7252812b7348d905b},
  Intrahash                = {42ec2a9ea2dc7238a0ca148152203ef1},
  Keywords                 = {gaussian process},
  Url                      = {http://arxiv.org/abs/1309.6835}
}

@Article{Dimple,
  Title                    = {Accelerating Inference: towards a full Language, Compiler and Hardware stack},
  Author                   = {Hershey, Shawn and Bernstein, Jeffrey and Bradley, Bill and Schweitzer, Andrew and Stein, Noah and Weber, Theophane and Vigoda, Benjamin},
  Journal                  = {Computing Research Repository},
  Year                     = {2012},

  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@Article{Hoffman:1991,
  Title                    = {Constrained realizations of {Gaussian} fields: a simple algorithm},
  Author                   = {Hoffman, Yehuda and Ribak, Erez},
  Journal                  = {The Astrophysical Journal},
  Year                     = {1991},
  Pages                    = {L5--L8},
  Volume                   = {380}
}

@Article{Honkela:2010,
  Title                    = {Approximate {Riemannian} Conjugate Gradient Learning for Fixed-Form Variational {Bayes}},
  Author                   = {Honkela, Antti and Raiko, Tapani and Kuusela, Mikael and Tornio, Matti and Karhunen, Juha},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2010},
  Pages                    = {3235--3268},
  Volume                   = {11},

  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@Article{Honkela:2003,
  Title                    = {Accelerating Cyclic Update Algorithms for Parameter Estimation by Pattern Searches},
  Author                   = {Honkela, Antti and Valpola, Harri and Karhunen, Juha},
  Journal                  = {Neural Processing Letters},
  Year                     = {2003},
  Number                   = {2},
  Pages                    = {191--203},
  Volume                   = {17},

  Doi                      = {10.1023/A:1023655202546}
}

@InProceedings{Ilin:2009,
  Title                    = {{Bayesian} {PCA} for Reconstruction of Historical Sea Surface Temperatures},
  Author                   = {Ilin, Alexander and Kaplan, Alexey},
  Booktitle                = {Proceedings of the International Joint Conference on Neural Networks (IJCNN 2009)},
  Year                     = {2009},
  Pages                    = {1322--1327}
}

@Article{Ilin:2010:PCA,
  Title                    = {Practical Approaches to Principal Component Analysis in the Presence of Missing Values},
  Author                   = {Ilin, Alexander and Raiko, Tapani},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2010},
  Pages                    = {1957--2000},
  Volume                   = {11}
}

@TechReport{Ilin:2008,
  Title                    = {Practical Approaches to Principal Component Analysis in the Presence of Missing Values},
  Author                   = {A. Ilin and T. Raiko},
  Institution              = {Helsinki University of Technology},
  Year                     = {2008},

  Address                  = {Espoo, Finland},
  Note                     = {Available at http://www.cis.hut.fi/alexilin/},
  Number                   = {TKK-ICS-R6}
}

@Article{Ilin:2005,
  Title                    = {On the Effect of the Form of the Posterior Approximation in Variational Learning of {ICA} Models},
  Author                   = {Ilin, Alexander and Valpola, Harri},
  Journal                  = {Neural Processing Letters},
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {183--204},
  Volume                   = {22}
}

@Article{Ilin:2006,
  Title                    = {Exploratory analysis of climate data using source separation methods},
  Author                   = {Ilin, Alexander and Valpola, Harri and Oja, Erkki},
  Journal                  = {Neural Networks},
  Year                     = {2006},
  Number                   = {2},
  Pages                    = {155--167},
  Volume                   = {19}
}

@Book{Jolliffe:2002,
  Title                    = {Principal Component Analysis},
  Author                   = {Jolliffe, Ian T.},
  Publisher                = {Springer},
  Year                     = {2002},

  Address                  = {New York},
  Edition                  = {2nd}
}

@InProceedings{Jordan:1998,
  Title                    = {An Introduction to Variational Methods for Graphical Methods},
  Author                   = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  Booktitle                = {Machine Learning},
  Year                     = {1998},
  Pages                    = {183--233},
  Publisher                = {MIT Press}
}

@Article{Jun:2007,
  Title                    = {An Approach to Producing Space–Time Covariance Functions on Spheres},
  Author                   = {Jun, Mikyoung and Stein, Michael L.},
  Journal                  = {Technometrics},
  Year                     = {2007},

  Month                    = {nov},
  Number                   = {4},
  Pages                    = {468--479},
  Volume                   = {49},

  Abstract                 = {For space–time processes on global or large scales, it is critical to use models that respect the Earth's spherical shape. The covariance functions of such processes should be not only positive definite on sphere × time, but also capable of capturing the dynamics of the processes well. We develop space–time covariance functions on sphere × time that are flexible in producing space–time interactions, especially space–time asymmetries. Our idea is to consider a sum of independent processes in which each process is obtained by applying a first-order differential operator to a fully symmetric process on sphere × time. The resulting covariance functions can produce various types of space–time interactions and give different covariance structures along different latitudes. Our approach yields explicit expressions for the covariance functions, which has great advantages in computation. Moreover, it applies equally well to generating asymmetric space–time covariance functions on flat or other spatial domains. We study various characteristics of our new covariance functions, focusing on their space–time interactions. We apply our model to a dataset of total column ozone levels in the Northern hemisphere.},
  Doi                      = {10.1198/004017007000000155}
}

@Book{Kaipio:2005,
  Title                    = {Statistical and Computational Inverse Problems},
  Author                   = {Kaipio, Jari and Somersalo, Erkki},
  Publisher                = {Springer},
  Year                     = {2005},
  Series                   = {Applied Mathematical Sciences},
  Volume                   = {160},

  Owner                    = {jluttine},
  Timestamp                = {2014.10.24}
}

@Article{Kalman:1961,
  Title                    = {New Results in Linear Filtering and Prediction Theory},
  Author                   = {Kalman, R. E. and Bucy, R. S.},
  Journal                  = {Journal of Basic Engineering},
  Year                     = {1961},
  Pages                    = {95--108},
  Volume                   = {85},

  Owner                    = {jluttine},
  Timestamp                = {2013.04.19}
}

@InProceedings{Kang:2012,
  Title                    = {Probabilistic Models for Common Spatial Patterns: Parameter-Expanded {EM} and Variational {Bayes}},
  Author                   = {Hyohyeong Kang and Seungjin Choi},
  Booktitle                = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
  Year                     = {2012},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/4839}
}

@Article{Kaplan:1998,
  Title                    = {Analyses of global sea surface temperature 1856--1991},
  Author                   = {Kaplan, Alexey and Cane, Mark A. and Kushnir, Yochanan and Clement, Amy C. and Blumenthal, M. Benno and Rajagopalan, Balaji},
  Journal                  = {Journal of Geophysical Research},
  Year                     = {1998},

  Month                    = aug,
  Number                   = {C9},
  Pages                    = {18567--18589},
  Volume                   = {103}
}

@TechReport{Khan:2004,
  Title                    = {Robust Generative Subspace Modeling: The Subspace t Distribution},
  Author                   = {Zia Khan and Frank Dellaert},
  Institution              = {GVU Center, College of Computing, Georgia},
  Year                     = {2004},

  Owner                    = {jluttine},
  Timestamp                = {2014.09.11}
}

@Article{Klami:2013,
  Title                    = {{Bayesian} canonical correlation analysis},
  Author                   = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2013},
  Pages                    = {899--937},
  Volume                   = {14},

  Owner                    = {jluttine},
  Timestamp                = {2013.04.19}
}

@InProceedings{Kozma:2009,
  Title                    = {Binary principal component analysis in the {N}etflix collaborative filtering task},
  Author                   = {L.~Kozma and A.~Ilin and T.~Raiko},
  Booktitle                = {Proceedings of the 2009 IEEE international workshop on machine learning for signal processing (MLSP~2009)},
  Year                     = {2008},
  Note                     = {To appear}
}

@InCollection{Lappalainen:2000b,
  Title                    = {{B}ayesian Nonlinear Independent Component Analysis by Multi-Layer Perceptrons},
  Author                   = {Lappalainen, Harri and Honkela, Antti},
  Booktitle                = {Advances in Independent Component Analysis},
  Publisher                = {Springer-Verlag},
  Year                     = {2000},

  Address                  = {Berlin},
  Editor                   = {M.~Girolami},
  Pages                    = {93--121}
}

@InCollection{Lappalainen:2000a,
  Title                    = {Ensemble Learning},
  Author                   = {Lappalainen, Harri and Miskin, James W.},
  Booktitle                = {Advances in Independent Component Analysis},
  Publisher                = {Springer-Verlag},
  Year                     = {2000},
  Editor                   = {M. Girolami},
  Pages                    = {75--92}
}

@Article{Lawrence:2005,
  Title                    = {Probabilistic Non-linear Principal Component Analysis with {Gaussian} Process Latent Variable Models},
  Author                   = {Neil D. Lawrence},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2005},
  Pages                    = {1783-1816},
  Volume                   = {6}
}

@Article{Lemos:2009,
  Title                    = {A spatio-temporal model for mean, anomaly and trend fields of {North} {Atlantic} sea surface temperature},
  Author                   = {Lemos, Ricardo T. and Sans\'{o}},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2009},
  Number                   = {485},
  Pages                    = {5--18},
  Volume                   = {104},

  Abstract                 = {We consider the problem of fitting a statistical model to 30 years of sea surface temperature records collected over a large portion of the Northern Atlantic. The observations were collected sparsely in space and time with different levels of accuracy. The purpose of the model is to produce an atlas of oceanic properties, including climatological mean fields, estimates of historical trends, and a spatio-temporal reconstruction of the anomalies, i.e., the transient deviations from the climatological mean. These products are of interest to climate change and climate variability research, numerical modeling, and remote sensing analyses. Our model improves upon the current tools used by oceanographers in that it constructs instantaneous temperature fields before averaging them into the climatology, thus giving equal weight to all years in the time frame, regardless of the temporal distribution of data. It also accounts for nonisotropic and nonstationary space and time dependencies, owing to its use of discrete process convolutions. Particular attention is given to the handling of massive datasets such as the one under study. This is achieved by considering compact support kernels that allow an efficient parallelization of the Markov chain Monte Carlo method used in the estimation of the model parameters. Resulting monthly climatologies are compared with those of the World Ocean Atlas 2001, version 2. Different water masses appear better separated in our climatology, and a close link emerges between the kernels' shape and the dominating patterns of ocean currents. The subpolar and the temperate North Atlantic display opposite trends, with the former mainly cooling over the years and the latter mainly warming, especially in the Gulf Stream region. Long-term changes in annual cycles are also detected. As in any hierarchical Bayesian model, parameter estimates come with credibility intervals, which are useful to compare results with other approaches and detect areas where sampling campaigns are needed the most.},
  Doi                      = {10.1198/jasa.2009.0018}
}

@Article{Liu:1995,
  Title                    = {{ML} estimation of the t distribution using {EM} and its extensions, {ECM} and {ECME}},
  Author                   = {Liu, C. and Rubin, D. B.},
  Journal                  = {Statistica Sinica},
  Year                     = {1995},
  Pages                    = {19--39},
  Volume                   = {5},

  Owner                    = {jluttine},
  Timestamp                = {2014.09.11}
}

@Article{Liu:1998,
  Title                    = {Parameter expansion to accelerate {EM:} the {PX-EM} algorithm},
  Author                   = {Liu, Chuanhai and Rubin, Donald B. and Wu, Ying Nian},
  Journal                  = {Biometrika},
  Year                     = {1998},
  Pages                    = {755--770},
  Volume                   = {85}
}

@Article{Liu:1999,
  Title                    = {Parameter Expansion for Data Augmentation},
  Author                   = {Liu, Jun S. and Wu, Ying N.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1999},
  Pages                    = {1264--1274},
  Volume                   = {94},

  Publisher                = {American Statistical Association}
}

@Article{Lopes:2008,
  Title                    = {Spatial Dynamic Factor Analysis},
  Author                   = {Lopes, Hedibert Freitas and Salazar, Esther and Gamerman, Dani},
  Journal                  = {Bayesian Analysis},
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {759--792},
  Volume                   = {3}
}

@InCollection{Luttinen:2013,
  Title                    = {Fast Variational {Bayesian} Linear State-Space Model},
  Author                   = {Luttinen, Jaakko},
  Booktitle                = {Machine Learning and Knowledge Discovery in Databases},
  Publisher                = {Springer},
  Year                     = {2013},
  Editor                   = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Železný, Filip},
  Pages                    = {305--320},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {8188},

  Abstract                 = {This paper presents a fast variational Bayesian method for linear state-space models. The standard variational Bayesian expectation-maximization (VB-EM) algorithm is improved by a parameter expansion which optimizes the rotation of the latent space. With this approach, the inference is orders of magnitude faster than the standard method. The speed of the proposed method is demonstrated on an artificial dataset and a large real-world dataset, which shows that the standard VB-EM algorithm is not suitable for large datasets because it converges extremely slowly. In addition, the paper estimates the temporal state variables using a smoothing algorithm based on the block LDL decomposition. This smoothing algorithm reduces the number of required matrix inversions and avoids a model augmentation compared to previous approaches.},
  Doi                      = {10.1007/978-3-642-40988-2_20},
  File                     = {:Luttinen\:2013.pdf:PDF}
}

@Misc{bayespy.org,
  Title                    = {{BayesPy} Online Documentation},

  Author                   = {Luttinen, Jaakko},
  HowPublished             = {\url{http://bayespy.org}},
  Year                     = {2014},

  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@InProceedings{Luttinen:2012:aistats,
  Title                    = {Efficient {Gaussian} Process Inference for Short-Scale Spatio-Temporal Modeling},
  Author                   = {Luttinen, Jaakko and Ilin, Alexander},
  Booktitle                = {JMLR Workshop and Conference Proceedings (AISTATS 2012)},
  Year                     = {2012},
  Pages                    = {741--750},
  Volume                   = {22},

  Abstract                 = {This paper presents an efficient Gaussian process inference scheme for modeling shortscale phenomena in spatio-temporal datasets. Our model uses a sum of separable, compactly supported covariance functions, which yields a full covariance matrix represented in terms of small sparse matrices operating either on the spatial or temporal domain. The proposed inference procedure is based on Gibbs sampling, in which samples from the conditional distribution of the latent function values are obtained by applying a simple linear transformation to samples drawn from the joint distribution of the function values and the observations. We make use of the proposed model structure and the conjugate gradient method to compute the required transformation. In the experimental part, the proposed algorithm is compared to the standard approach using the sparse Cholesky decomposition and it is shown to be much faster and computationally feasible for 100-1000 times larger datasets. We demonstrate the advantages of the proposed method in the problem of reconstructing sea surface temperature, which requires processing of a real-world dataset with 10^6 observations.},
  File                     = {:Luttinen\:2012\:aistats.pdf:PDF},
  Url                      = {http://jmlr.csail.mit.edu/proceedings/papers/v22/luttinen12/luttinen12.pdf}
}

@Article{Luttinen:2010,
  Title                    = {Transformations in variational {Bayesian} factor analysis to speed up learning},
  Author                   = {Luttinen, Jaakko and Ilin, Alexander},
  Journal                  = {Neurocomputing},
  Year                     = {2010},
  Pages                    = {1093--1102},
  Volume                   = {73},

  Abstract                 = {We propose simple transformation of the hidden states in variational Bayesian factor analysis models to speed up the learning procedure. The speed-up is achieved by using proper parameterization of the posterior approximation which allows joint optimization of its individual factors, thus the transformation is theoretically justified. We derive the transformation formulae for variational Bayesian factor analysis and show experimentally that it can significantly improve the rate of convergence. The proposed transformation basically performs centering and whitening of the hidden factors taking into account the posterior uncertainties. Similar transformations can be applied to other variational Bayesian factor analysis models as well.},
  Doi                      = {10.1016/j.neucom.2009.11.018},
  File                     = {:Luttinen\:2010.pdf:PDF}
}

@Article{Luttinen:2012:npl,
  Title                    = {{Bayesian} Robust {PCA} of Incomplete Data},
  Author                   = {Luttinen, Jaakko and Ilin, Alexander and Karhunen, Juha},
  Journal                  = {Neural Processing Letters},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {189--202},
  Volume                   = {36},

  Abstract                 = {We present a probabilistic model for robust factor analysis and principal component analysis in which the observation noise is modeled by Student- t distributions in order to reduce the negative effect of outliers. The Student- t distributions are modeled independently for each data dimensions, which is different from previous works using multivariate Student- t distributions. We compare methods using the proposed noise distribution, the multivariate Student- t and the Laplace distribution. Intractability of evaluating the posterior probability density is solved by using variational Bayesian approximation methods. We demonstrate that the assumed noise model can yield accurate reconstructions because corrupted elements of a bad quality sample can be reconstructed using the other elements of the same data vector. Experiments on an artificial dataset and a weather dataset show that the dimensional independency and the flexibility of the proposed Student- t noise model can make it superior in some applications.},
  Doi                      = {10.1007/s11063-012-9230-4},
  File                     = {:Luttinen\:2012\:npl.pdf:PDF}
}

@InCollection{Luttinen:2014,
  Title                    = {Linear State-Space Model with Time-Varying Dynamics},
  Author                   = {Luttinen, Jaakko and Raiko, Tapani and Ilin, Alexander},
  Booktitle                = {Machine Learning and Knowledge Discovery in Databases},
  Publisher                = {Springer},
  Year                     = {2014},
  Editor                   = {Calders, Toon and Esposito, Floriana and H\"ullermeier, Eyke and Meo, Rosa},
  Note                     = {To appear},
  Pages                    = {338--353},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {8725},

  Abstract                 = {This paper introduces a linear state-space model with time-varying dynamics. The time dependency is obtained by forming the state dynamics matrix as a time-varying linear combination of a set of matrices. The time dependency of the weights in the linear combination is modelled by another linear Gaussian dynamical model allowing the model to learn how the dynamics of the process changes. Previous approaches have used switching models which have a small set of possible state dynamics matrices and the model selects one of those matrices at each time, thus jumping between them. Our model forms the dynamics as a linear combination and the changes can be smooth and more continuous. The model is motivated by physical processes which are described by linear partial differential equations whose parameters vary in time. An example of such a process could be a temperature field whose evolution is driven by a varying wind direction. The posterior inference is performed using variational Bayesian approximation. The experiments on stochastic advection-diffusion processes and real-world weather processes show that the model with time-varying dynamics can outperform previously introduced approaches.},
  Doi                      = {10.1007/978-3-662-44851-9_22}
}

@InCollection{MacKay:1998,
  Title                    = {Introduction to {Gaussian} Processes},
  Author                   = {MacKay, David J. C.},
  Booktitle                = {Neural Networks and Machine Learning},
  Publisher                = {Springer},
  Year                     = {1998},
  Editor                   = {Bishop, Christopher M.},
  Pages                    = {133--166}
}

@Article{MacKay:1999,
  Title                    = {Comparison of Approximate Methods for Handling Hyperparameters},
  Author                   = {MacKay, David J. C.},
  Journal                  = {Neural Computation},
  Year                     = {1999},
  Pages                    = {1035--1068},
  Volume                   = {11}
}

@Misc{Infer.NET,
  Title                    = {{Infer.NET 2.5}},

  Author                   = {Minka, Tom and Winn, John and Guiver, John and Knowles, David A.},
  Note                     = {Microsoft Research Cambridge. http://research.microsoft.com/infernet},
  Year                     = {2012}
}

@InCollection{Neal:2011,
  Title                    = {{MCMC} using {Hamiltonian} dynamics},
  Author                   = {Neal, Radford M.},
  Booktitle                = {Handbook of {Markov} Chain {Monte} {Carlo}},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {2011},
  Editor                   = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li Meng}
}

@Article{Neal:2003,
  Title                    = {Slice sampling},
  Author                   = {Neal, Radford M.},
  Journal                  = {The Annals of Statistics},
  Year                     = {2003},
  Number                   = {3},
  Pages                    = {705--767},
  Volume                   = {31}
}

@TechReport{Neal:1993,
  Title                    = {Probabilistic Inference Using {Markov} Chain {Monte} {Carlo} Methods},
  Author                   = {Neal, Radford M.},
  Institution              = {Department of Computer Science, University of Toronto},
  Year                     = {1993}
}

@Article{Niessner:1983,
  Title                    = {On computing the inverse of a sparse matrix},
  Author                   = {Niessner, H. and Reichert, K.},
  Journal                  = {International Journal for Numerical Methods in Engineering},
  Year                     = {1983},
  Pages                    = {1513--1526},
  Volume                   = {19}
}

@Article{Parker:2011,
  Title                    = {Sampling {Gaussian} distributions in {Krylov} spaces with conjugate gradients},
  Author                   = {Parker, Albert and Fox, Colin},
  Note                     = {under review}
}

@Article{PyMC,
  Title                    = {{PyMC:} {Bayesian} Stochastic Modelling in {Python}},
  Author                   = {Patil, Anand and Huard, David and Fonnesbeck, Christopher J.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {1--81},
  Volume                   = {35},

  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@InCollection{Pavlovic:2000,
  Title                    = {Learning Switching Linear Models of Human Motion},
  Author                   = {Pavlovic, Vladimir and Rehg, James M. and MacCormick, John},
  Booktitle                = {Advances in Neural Information Processing Systems 13},
  Publisher                = {MIT Press},
  Year                     = {2001},
  Pages                    = {981--987},

  Owner                    = {jluttine},
  Timestamp                = {2014.09.11}
}

@Article{Pearson:1901,
  Title                    = {On lines and planes of closest fit to systems of points in space},
  Author                   = {Pearson, Karl},
  Journal                  = {Philosophical Magazine},
  Year                     = {1901},
  Number                   = {6},
  Pages                    = {559--572},
  Volume                   = {2}
}

@Misc{Petersen:2008,
  Title                    = {The Matrix Cookbook},

  Author                   = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  Month                    = oct,
  Note                     = {Version 20081110},
  Year                     = {2008},

  Publisher                = {Technical University of Denmark},
  Url                      = {http://www2.imm.dtu.dk/pubdb/p.php?3274}
}

@PhdThesis{Puggioni:2009,
  Title                    = {Using Data Augmentation and Stochastic Differential Equations in Spatio Temporal Modeling},
  Author                   = {Puggioni, Gavino},
  School                   = {Duke University},
  Year                     = {2009}
}

@Article{Quinonero-Candela:2005,
  Title                    = {A Unifying View of Sparse Approximate {Gaussian} Process Regression},
  Author                   = {Qui\~nonero{-}Candela, Joaquin and Rasmussen, Carl Edward},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2005},

  Month                    = dec,
  Pages                    = {1939--1959},
  Volume                   = {6}
}

@InProceedings{raiko2010drifting,
  Title                    = {Drifting Linear Dynamics (Abstract)},
  Author                   = {Raiko, Tapani and Ilin, Alexander and Korsakova, Natalia and Oja, Erkki and Valpola, Harri},
  Booktitle                = {International Conference on Artificial Intelligence and Statistics (AISTATS 2010)},
  Year                     = {2010},

  Owner                    = {jluttine},
  Timestamp                = {2014.09.11}
}

@Article{Raiko:2007,
  Title                    = {Building blocks for variational {Bayesian} learning of latent variable models},
  Author                   = {Raiko, Tapani and Valpola, Harri and Harva, Markus and Karhunen, Juha},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2007},

  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@Book{Rasmussen:2006,
  Title                    = {Gaussian Processes for Machine Learning},
  Author                   = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  Publisher                = {MIT Press},
  Year                     = {2006}
}

@Article{Rauch:1965,
  Title                    = {Maximum likelihood estimates of linear dynamic systems},
  Author                   = {Rauch, H. E. and Tung, F. and Striebel, C. T.},
  Journal                  = {AIAA Journal},
  Year                     = {1965},
  Number                   = {8},
  Pages                    = {1445--1450},
  Volume                   = {3},

  Owner                    = {jluttine},
  Timestamp                = {2013.04.19}
}

@Article{Rougier:2008,
  Title                    = {Efficient Emulators for Multivariate Deterministic Functions},
  Author                   = {Rougier, Jonathan},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2008},

  Month                    = {dec},
  Number                   = {4},
  Pages                    = {827--843},
  Volume                   = {17},

  Abstract                 = {One of the challenges with emulating the response of a multivariate function to its inputs is the quantity of data that must be assimilated, which is the product of the number of model evaluations and the number of outputs. This article shows how even large calculations can be made tractable. It is already appreciated that gains can be made when the emulator residual covariance function is treated as separable in the model-inputs and model-outputs. Here, an additional simplification on the structure of the regressors in the emulator mean function allows very substantial further gains. The result is that it is now possible to emulate rapidly—on a desktop computer—models with hundreds of evaluations and hundreds of outputs. This is demonstrated through calculating costs in floating-point operations, and in an illustration. Even larger sets of outputs are possible if they have additional structure, for example, spatial-temporal.},
  Doi                      = {10.1198/106186008X384032}
}

@Book{Saad:2003,
  Title                    = {Iterative Methods for Sparse Linear Systems},
  Author                   = {Saad, Yousef},
  Year                     = {2003}
}

@Article{Schmidt:2008,
  Title                    = {Nonnegative matrix factorization with {Gaussian} process priors},
  Author                   = {Schmidt, Mikkel N. and Laurberg, Hans},
  Journal                  = {Computational Intelligence and Neuroscience},
  Year                     = {2008},
  Pages                    = {1--10},
  Volume                   = {2008},

  Address                  = {New York, NY, United States},
  Publisher                = {Hindawi Publishing Corporation}
}

@TechReport{Shewchuk:1994,
  Title                    = {An introduction to the conjugate gradient method without the agonizing pain},
  Author                   = {Shewchuk, Jonathan Richard},
  Institution              = {Carnegie Mellon University},
  Year                     = {1994}
}

@Book{Shumway:2000,
  Title                    = {Time Series Analysis and Its Applications},
  Author                   = {Robert H. Shumway and David S. Stoffer},
  Publisher                = {Springer},
  Year                     = {2000},

  Owner                    = {jluttine},
  Timestamp                = {2013.04.19}
}

@Article{Sinnott:1984,
  Title                    = {Virtues of the Haversine},
  Author                   = {Sinnott, Roger W.},
  Journal                  = {Sky and Telescope},
  Year                     = {1984},
  Number                   = {2},
  Pages                    = {159},
  Volume                   = {68}
}

@Article{Smith:1995,
  Title                    = {Differentiation of the {Cholesky} Algorithm},
  Author                   = {Smith, S. P.},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {1995},
  Number                   = {2},
  Pages                    = {134--147},
  Volume                   = {4}
}

@Misc{Stan,
  Title                    = {Stan: A C++ Library for Probability and Sampling, Version 2.4},

  Author                   = {{Stan Development Team}},
  Year                     = {2014},

  Url                      = {http://mc-stan.org/}
}

@Article{Stein:2005,
  Title                    = {Space-time covariance functions},
  Author                   = {Stein, Michael L.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2005},
  Number                   = {469},
  Pages                    = {310--321},
  Volume                   = {100}
}

@Book{vonStorch:1999,
  Title                    = {Statistical analysis in climate research},
  Author                   = {von Storch, H. and Zwiers, W.},
  Publisher                = {Cambridge University Press},
  Year                     = {1999},

  Address                  = {Cambridge, UK}
}

@InProceedings{Takahashi:1973,
  Title                    = {Formation of a sparse bus impedance matrix and its application to short circuit study},
  Author                   = {Takahashi, K. and Fagan, J. and Chen, M.-S.},
  Booktitle                = {Power Industry Computer Application Conference Proceedings},
  Year                     = {1973}
}

@Article{OpenBUGS,
  Title                    = {Making {BUGS} Open},
  Author                   = {Thomas, Andrew and O'Hara, Bob and Ligges, Uwe and Sibylle, Sturtz},
  Journal                  = {R News},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {12--17},
  Volume                   = {6},

  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@Article{Tipping:1999,
  Title                    = {Probabilistic Principal Component Analysis},
  Author                   = {Tipping, Michael E. and Bishop, Christopher M.},
  Journal                  = {Journal of the Royal Statistical Society Series B},
  Year                     = {1999},
  Number                   = {3},
  Pages                    = {611--622},
  Volume                   = {61}
}

@Article{Valpola:2002,
  Title                    = {An unsupervised ensemble learning method for nonlinear dynamic state-space models},
  Author                   = {Valpola, Harri and Karhunen, Juha},
  Journal                  = {Neural computation},
  Year                     = {2002},
  Number                   = {11},
  Pages                    = {2647--2692},
  Volume                   = {14},

  Owner                    = {jluttine},
  Publisher                = {MIT Press},
  Timestamp                = {2014.09.02}
}

@Book{Wackernagel:2003,
  Title                    = {Multivariate Geostatistics},
  Author                   = {Hans Wackernagel},
  Publisher                = {Springer Science \& Business Media},
  Year                     = {2003},
  Edition                  = {3rd},

  Owner                    = {jluttine},
  Timestamp                = {2014.10.22}
}

@Misc{Walsh:2004,
  Title                    = {Markov Chain Monte Carlo and Gibbs Sampling},

  Author                   = {Walsh, B.}
}

@InCollection{Wikle:2006,
  Title                    = {Hierarchical {Bayesian} spatio–temporal models for population spread},
  Author                   = {Wikle, Christopher K. and Hooten, Mevin B.},
  Booktitle                = {Hierarchical Modelling for the Environmental Sciences: Statistical Methods and Applications},
  Publisher                = {Oxford University Press},
  Year                     = {2006},
  Chapter                  = {8},
  Editor                   = {Clark, James S. and Gelfand, Alan},
  Pages                    = {145--169},

  Abstract                 = {There is a long history in the ecological sciences concerning the development of mathematical models for describing the distribution of organisms over time and space. Although such models have sometimes been evaluated by comparison to observations, they have seldom been “fit” to data in a formal statistical sense. Thus, uncertainties regarding data, model, and parameters are not readily accounted for in such analyses. Alternatively, realistic statistical models for spatio–temporal processes in ecology often require that one estimate a very large number of parameters. This is typically not possible given the relatively limited amount of data collected over space and time. Critically, such a statistical model could be simplified (in terms of reducing the number of parameters) by accommodating the well known empirical and theoretical results concerning the process (e.g. partial differential equations (PDEs) or integro–difference equations (IDEs) for population spread). That is, the statistical model can make use of the well established mathematical models for the process. The Bayesian hierarchical paradigm for spatio–temporal models allows one to account for the aforementioned sources of uncertainty and yet still include such prior knowledge for the process, parameters and measurements. In this chapter, we provide an introduction to process-based hierarchical Bayesian spatio–temporal models for population spread, focusing primarily on PDE dynamics. We illustrate the concepts and demonstrate the methodology on the problem of predicting the Eurasian Collared-Dove invasion of North America.}
}

@Article{Wikle:2001,
  Title                    = {Spatio-Temporal Hierarchical {Bayesian} Modeling: Tropical Ocean Surface Winds},
  Author                   = {Wikle, Christopher K. and Milliff, Ralph F. and Nychka, Doug and Berliner, L. Mark},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2001},
  Pages                    = {382--397},
  Volume                   = {96}
}

@InProceedings{Wilson:2011,
  Title                    = {Generalised {Wishart} Processes},
  Author                   = {Wilson, Andrew Gordon and Ghahramani, Zoubin},
  Booktitle                = {Proceedings of the Proceedings of the Twenty-Seventh Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI 2011)},
  Year                     = {2011},

  Address                  = {Corvallis, Oregon},
  Pages                    = {736-744},
  Publisher                = {AUAI Press},

  Abstract                 = {We introduce a new stochastic process called the generalised Wishart process (GWP). It is a collection of positive semi-definite random matrices indexed by any arbitrary input variable. We use this process as a prior over dynamic (e.g. time varying) covariance matrices. The GWP captures a diverse class of covariance dynamics, naturally handles missing data, scales nicely with dimension, has easily interpretable parameters, and can use input variables that include covariates other than time. We describe how to construct the GWP, introduce general procedures for inference and prediction, and show that it outperforms its main competitor, multivariate GARCH, even on financial data that especially suits GARCH.}
}

@Article{Winn:2005,
  Title                    = {Variational Message Passing},
  Author                   = {Winn, John and Bishop, Christopher M.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2005},
  Pages                    = {661--694},
  Volume                   = {6},

  Abstract                 = {Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Message Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be specified graphically and then solved variationally without recourse to coding.},
  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@Article{Zastavnyi:2011,
  Title                    = {Characterization theorems for the {Gneiting} class of space-time covariances},
  Author                   = {Zastavnyi, Viktor P. and Porcu, Emilio},
  Journal                  = {Bernoulli},
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {456--465},
  Volume                   = {17},

  Abstract                 = {We characterize the Gneiting class of space–time covariance functions and give more relaxed conditions on the functions involved. We then show necessary conditions for the construction of compactly supported functions of the Gneiting type. These conditions are very general since they do not depend on the Euclidean norm.},
  Doi                      = {10.3150/10-BEJ278}
}

@Article{Zhang:2007,
  Title                    = {Maximum-likelihood estimation for multivariate spatial linear coregionalization models},
  Author                   = {Zhang, Hao},
  Journal                  = {Environmetrics},
  Year                     = {2007},
  Number                   = {2},
  Pages                    = {125--139},
  Volume                   = {18},

  Doi                      = {10.1002/env.807}
}

@Article{Zhao:2006,
  Title                    = {Probabilistic {PCA} for t distributions},
  Author                   = {Zhao, J. and Jiang, Q.},
  Journal                  = {Neurocomputing},
  Year                     = {2006},

  Month                    = {October},
  Pages                    = {2217--2226},
  Volume                   = {69},

  Owner                    = {jluttine},
  Timestamp                = {2014.09.11}
}

@Book{NIPS:2002,
  Title                    = {Advances in Neural Information Processing Systems 15},
  Editor                   = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
  Publisher                = {MIT Press},
  Year                     = {2002},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 15}
}

@Book{NIPS:2009,
  Title                    = {Advances in Neural Information Processing Systems 22},
  Editor                   = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. K. I. and Culotta, A.},
  Publisher                = {MIT Press},
  Year                     = {2009},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 22}
}

@Proceedings{AISTATS:2003,
  Year                     = {2003},
  Editor                   = {Bishop, Christopher M. and Frey, Brendan J.},
  Publisher                = {Society for Artificial Intelligence and Statistics},

  Booktitle                = {Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics (AISTATS'03)}
}

@Proceedings{ICML:2009,
  Title                    = {Proceedings of the 26th International Conference on Machine Learning (ICML'09)},
  Year                     = {2009},

  Address                  = {Montreal},
  Editor                   = {L\'{e}on Bottou and Michael Littman},
  Month                    = jun,
  Publisher                = {Omnipress},

  Booktitle                = {Proceedings of the 26th International Conference on Machine Learning (ICML'09)}
}

@Proceedings{AISTATS:2005,
  Year                     = {2005},
  Editor                   = {Cowell, Robert G. and Ghahramani, Zoubin},
  Publisher                = {Society for Artificial Intelligence and Statistics},

  Booktitle                = {Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics (AISTATS'05)}
}

@Book{NIPS:2001,
  Title                    = {Advances in Neural Information Processing Systems 14},
  Editor                   = {Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin},
  Publisher                = {MIT Press},
  Year                     = {2001},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 14}
}

@Proceedings{AISTATS:2009,
  Year                     = {2009},
  Editor                   = {van Dyk, David and Welling, Max},
  Publisher                = {Society for Artificial Intelligence and Statistics},

  Booktitle                = {Proceedings of the 12th International Workshop on Artificial Intelligence and Statistics (AISTATS'09)}
}

@Book{Finkenstadt:2007,
  Title                    = {Statistical Methods for Spatio-Temporal Systems},
  Editor                   = {Finkenst\"adt, B\"arbel and Held, Leonhard and Isham, Valerie},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {2007},
  Number                   = {107},
  Series                   = {Monographs on Statistics and Applied Probability}
}

@Book{NIPS:1998,
  Title                    = {Advances in Neural Information Processing Systems 11},
  Editor                   = {Kearns, M. J. and Solla, S. A. and Cohn, D. A.},
  Publisher                = {MIT Press},
  Year                     = {1999},

  Booktitle                = {Advances in Neural Information Processing Systems 11}
}

@Book{NIPS:2008,
  Title                    = {Advances in Neural Information Processing Systems 21},
  Editor                   = {Koller, Daphne and Schuurmans, Dale and Bengio, Yoshua and Bottou, L\'eon},
  Publisher                = {MIT Press},
  Year                     = {2009},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 21}
}

@Book{NIPS:2010,
  Title                    = {Advances in Neural Information Processing Systems 23},
  Editor                   = {Lafferty, J. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel,R.S. and Culotta, A.},
  Publisher                = {MIT Press},
  Year                     = {2010},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 23},
  Owner                    = {jluttine},
  Timestamp                = {2014.08.13}
}

@Book{NIPS:2000,
  Title                    = {Advances in Neural Information Processing Systems 13},
  Editor                   = {Leen, Todd K. and Dietterich Thomas G. and Tresp, Volker},
  Publisher                = {MIT Press},
  Year                     = {2001},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 13}
}

@Proceedings{UAI:2008,
  Title                    = {Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence (UAI'2008)},
  Year                     = {2008},

  Address                  = {Helsinki, Finland},
  Editor                   = {McAllester, David A. and Myllym{\"a}ki, Petri},
  Month                    = jul,
  Publisher                = {AUAI Press},

  Booktitle                = {Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence (UAI'2008)}
}

@Proceedings{AISTATS:2007,
  Year                     = {2007},
  Editor                   = {Meila, Marina and Shen, Xiaotong},
  Publisher                = {Society for Artificial Intelligence and Statistics},

  Booktitle                = {Proceedings of the 11th International Workshop on Artificial Intelligence and Statistics (AISTATS'07)}
}

@Book{NIPS:2012,
  Title                    = {Advances in Neural Information Processing Systems 25},
  Editor                   = {Pereira, F. and Burges, C.J.C. and Bottou, L. and Weinberger, K.Q.},
  Publisher                = {MIT Press},
  Year                     = {2012},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 23}
}

@Book{NIPS:2007,
  Title                    = {Advances in Neural Information Processing Systems 20},
  Editor                   = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S.},
  Publisher                = {MIT Press},
  Year                     = {2008},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 20}
}

@Book{NIPS:2004,
  Title                    = {Advances in Neural Information Processing Systems 17},
  Editor                   = {Saul, Lawrence K. and Weiss, Yair and Bottou, L\´eon},
  Publisher                = {MIT Press},
  Year                     = {2004},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 17}
}

@Book{NIPS:2006,
  Title                    = {Advances in Neural Information Processing Systems 19},
  Editor                   = {Sch\"olkopf, Bernhard and Platt, John and Hoffman, Thomas},
  Publisher                = {MIT Press},
  Year                     = {2007},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 19}
}

@Book{NIPS:1999,
  Title                    = {Advances in Neural Information Processing Systems 12},
  Editor                   = {Solla, Sara A. and Leen, Todd K. and M\"{u}ller, Klaus-Robert},
  Publisher                = {MIT Press},
  Year                     = {2000},

  Booktitle                = {Advances in Neural Information Processing Systems 12}
}

@Book{NIPS:2003,
  Title                    = {Advances in Neural Information Processing Systems 16},
  Editor                   = {Thrun, Sebastian and Saul, Lawrence K. and Sch\"olkopf, Bernhard},
  Publisher                = {MIT Press},
  Year                     = {2003},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 16}
}

@Book{NIPS:1995,
  Title                    = {Advances in Neural Information Processing Systems 8},
  Editor                   = {Touretzky, David S. and Mozer, Michael C. and Hasselmo, Michael E.},
  Publisher                = {MIT Press},
  Year                     = {1996},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 8}
}

@Proceedings{ESANN:2009,
  Year                     = {2009},
  Editor                   = {Verleysen, Michel},
  Publisher                = {d-side},

  Booktitle                = {Proceedings of the 17th European Symposium on Artificial Neural Networks (ESANN'2009)}
}

@Book{NIPS:2005,
  Title                    = {Advances in Neural Information Processing Systems 18},
  Editor                   = {Weiss, Yair and Sch\"olkopf, Bernhard and Platt, John},
  Publisher                = {MIT Press},
  Year                     = {2005},

  Address                  = {Cambridge, MA},

  Booktitle                = {Advances in Neural Information Processing Systems 18}
}

@InProceedings{Agovic:2011,
  Title                    = {Probabilistic Matrix Addition},

  Abstract                 = {We introduce Probabilistic Matrix Addition (PMA) for modeling real-valued data matrices by simultaneously capturing covariance structure among rows and among columns. PMA additively combines two latent matrices drawn from two Gaussian Processes respectively over rows and columns. The resulting joint distribution over the observed matrix does not factorize over entries, rows, or columns, and can thus capture intricate dependencies in the matrix. Exact inference in PMA is possible, but involves inversion of large matrices, and can be computationally prohibitive. Efficient approximate inference is possible due to the sparse dependency structure among latent variables. We propose two families of approximate inference algorithms for PMA based on Gibbs sampling and MAP inference. We demonstrate the effectiveness of PMA for missing value prediction and multi-label classification problems.}
}

@Article{Barry:1999,
  Title                    = {{Monte} {Carlo} estimates of the log determinant of large sparse matrices}
}

@Article{Chen:2011,
  Title                    = {Computing $f(A)b$ via least squares polynomial approximations}
}

@TechReport{Huang:2009,
  Title                    = {On the Validity of Covariance and Variogram Functions on the Sphere}
}

@Article{Kaplan:1997,
}

@InProceedings{Lazaro-Gredilla:2011,
  Title                    = {Variational Heteroscedastic {Gaussian} Process Regression},

  Abstract                 = {Standard Gaussian processes (GPs) model observations’ noise as constant throughout input space. This is often a too restrictive assumption, but one that is needed for GP inference to be tractable. In this work we present a non-standard variational approximation that allows accurate inference in heteroscedastic GPs (i.e., under input-dependent noise conditions). Computational cost is roughly twice that of the standard GP, and also scales as O($n^3$). Accuracy is verified by comparing with the golden standard MCMC and its effectiveness is illustrated on several synthetic and real datasets of diverse characteristics. An application to volatility forecasting is also considered.}
}

@Article{Lopes:2011,
  Title                    = {Generalized Spatial Dynamic Factor Models}
}

@Electronic{McCourt:2008,
  Title                    = {A Stochastic Simulation for Approximating the log-Determinant of a Symmetric Positive Definite Matrix}
}

@Article{Paciorek:2007,
  Title                    = {{Bayesian} Smoothing with {Gaussian} Processes Using {Fourier} Basis Functions in the {spectralGP} Package}
}

@TechReport{WHOWHATWHERE,
}

@Article{Zhang:2008,
  Title                    = {Log-det approximation based on uniformly distributed seeds and its application to {Gaussian} process regression}
}

@Article{,
}

@Proceedings{AISTATS:2011,
  Title                    = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  Year                     = {2011}
}

@Article{Karspeck:2011,
  Title                    = {{Bayesian} modelling and ensemble reconstruction of mid-scale spatial variability in {North} {Atlantic} sea-surface temperatures for 1850--2008},
  Journal                  = {Quarterly Journal of the Royal Meteorological Society},
  Year                     = {2011},

  Abstract                 = {Existing historical records of sea-surface temperature extending back to the mid-1800s are a valuable source of information about climate variability on interannual and decadal time-scales. However, the temporal and spatial irregularity of these data make them difficult to use in climate research, where gridded and complete data fields are expected for both statistical analysis and forcing numerical models. Infilling methods based on constraining the solution to the linear space spanned by the leading eigenvectors of the global-scale covariance, otherwise known as reduced-space methods, have proven very successful in creating gridded estimates of sea-surface temperature. These methods are especially useful for infilling the vast regions of unobserved ocean typical of the earliest segments of the data record. Regional variability, on the other hand, is not well represented by these methods, especially in data-poor regions. Here we present a method for augmenting the established large-scale reconstruction methods with a statistical model of the mid-scale variability. Using high quality sea-surface temperature data from the last 30 years including satellite-derived records, we specify a spatially non-stationary, anisotropic covariance model for the mid-scale sea-surface temperature variability. With the parameters of the covariance model estimated from the modern record, historical observations are used for conditioning the posterior distribution. Specifically, we form the expected value and correlated uncertainty of the mid-scales as well as generating samples from the posterior. While this work focuses on a limited domain in the midlatitude North Atlantic Ocean, the method employed here can be extended to global reconstructions.},
  Doi                      = {10.1002/qj.900}
}

@Article{Sun:2011,
  Title                    = {Geostatistics for Large Datasets},
  Year                     = {2011},

  Abstract                 = {We review various approaches for the geostatistical analysis of large datasets. First, we consider covariance structures that yield computational simplifications in geostatistics and briefly discuss how to test the suitability of such structures. Second, we describe the use of covariance tapering for both estimation and kriging purposes. Third, we consider likelihood approximations in both spatial and spectral domains. Fourth, we explore methods based on latent processes, such as Gaussian predictive processes and fixed rank kriging. Fifth, we describe methods based on Gaussian Markov random field approximations. Finally, we discuss multivariate extensions and open problems in this area.}
}

@Article{Konukoglu:2010,
  Title                    = {Image Guided Personalization of Reaction-Diffusion Type Tumor Growth Models Using Modified Anisotropic {Eikonal} Equations},
  Journal                  = {IEEE Transactions on Medical Imaging},
  Year                     = {2010},
  Pages                    = {77--95},
  Volume                   = {29},

  Abstract                 = {Reaction-diffusion based tumor growth models have been widely used in the literature for modeling the growth of brain gliomas. Lately, recent models have started integrating medical images in their formulation. Including different tissue types, geometry of the brain and the directions of white matter fiber tracts improved the spatial accuracy of reaction-diffusion models. The adaptation of the general model to the specific patient cases on the other hand has not been studied thoroughly yet. In this paper, we address this adaptation. We propose a parameter estimation method for reaction-diffusion tumor growth models using time series of medical images. This method estimates the patient specific parameters of the model using the images of the patient taken at successive time instances. The proposed method formulates the evolution of the tumor delineation visible in the images based on the reaction-diffusion dynamics; therefore, it remains consistent with the information available. We perform thorough analysis of the method using synthetic tumors and show important couplings between parameters of the reaction-diffusion model. We show that several parameters can be uniquely identified in the case of fixing one parameter, namely the proliferation rate of tumor cells. Moreover, regardless of the value the proliferation rate is fixed to, the speed of growth of the tumor can be estimated in terms of the model parameters with accuracy. We also show that using the model-based speed, we can simulate the evolution of the tumor for the specific patient case. Finally, we apply our method to two real cases and show promising preliminary results.},
  Doi                      = {10.1109/TMI.2009.2026413}
}

@Proceedings{ICA:2009,
  Year                     = {2009},
  Publisher                = {Springer-Verlag},

  Booktitle                = {Proceedings of the 8th International Conference on Independent Component Analysis and Signal Separation (ICA'2009)}
}

@Proceedings{ICIP:2007,
  Year                     = {2007},
  Month                    = sep,
  Publisher                = {IEEE},

  Booktitle                = {Proceedings of the International Conference on Image Processing (ICIP'2007)}
}

@Proceedings{ICML:2006,
  Year                     = {2006},

  Booktitle                = {Proceedings of the 23rd International Conference on Machine Learning (ICML'06)}
}

@Proceedings{ICANN:1999,
  Year                     = {1999},

  Booktitle                = {Proceedings of the 9th International Conference on Artificial Neural Networks (ICANN'99)}
}

@Article{Hasselmann:1976,
  Title                    = {Stochastic climate models},
  Year                     = {1976}
}

